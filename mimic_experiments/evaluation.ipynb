{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "from bert_score import BERTScorer\n",
    "from rouge_score.rouge_scorer import RougeScorer\n",
    "from scispacy.optimize import linear_sum_assignment\n",
    "\n",
    "from discharge_summaries.schemas.mimic import BHC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIMIC_III_DIR = (\n",
    "    Path.cwd().parent / \"data\" / \"physionet.org\" / \"files\" / \"mimiciii\" / \"1.4\"\n",
    ")\n",
    "BHC_FPATH = MIMIC_III_DIR / \"BHCS.json\"\n",
    "\n",
    "SNOMED_DIR = Path.cwd().parent / \"data\" / \"snomed\"\n",
    "TUNED_PHRASE_MATCHER_FPATH = SNOMED_DIR / \"tuned_snomed_phrase_matcher.pkl\"\n",
    "EXAMPLE_DIR = Path.cwd() / \"example\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snomed_phrase_matcher = pickle.load(TUNED_PHRASE_MATCHER_FPATH.open(\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bhcs = [BHC(**bhc_dict) for bhc_dict in json.loads(BHC_FPATH.read_text())]\n",
    "gt_bhc = bhcs[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_bhc = BHC(**json.loads((EXAMPLE_DIR / \"gpt_bhc.json\").read_text()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_scorer = BERTScorer(lang=\"en\", rescale_with_baseline=True)\n",
    "rouge_scorer = RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = rouge_scorer.score(gt_bhc.full_text, pred_bhc.full_text)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_scorer.score([gt_bhc.full_text], [pred_bhc.full_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_scorer.score([gt_bhc.assessment_and_plan], [pred_bhc.assessment_and_plan])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_scorer.score(gt_bhc.assessment_and_plan, pred_bhc.assessment_and_plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_bhc.assessment_and_plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_headings = [section.heading for section in gt_bhc.problem_sections]\n",
    "gt_headings_snomed_cuis = [\n",
    "    {ent.label_ for ent in snomed_phrase_matcher(heading).ents}\n",
    "    for heading in gt_headings\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_headings = [section.heading for section in pred_bhc.problem_sections]\n",
    "pred_headings_snomed_cuis = [\n",
    "    {ent.label_ for ent in snomed_phrase_matcher(heading).ents}\n",
    "    for heading in pred_headings\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_metric(gt, pred):\n",
    "    return len(gt.intersection(pred)) / len(gt.union(pred))\n",
    "\n",
    "\n",
    "cui_match_scores = np.array(\n",
    "    [\n",
    "        [jaccard_metric(pred_cuis, gt_cuis) for pred_cuis in pred_headings_snomed_cuis]\n",
    "        for gt_cuis in gt_headings_snomed_cuis\n",
    "    ]\n",
    ")\n",
    "gt_idxs, pred_idxs = linear_sum_assignment(cui_match_scores, maximize=True)\n",
    "cui_match_scores[gt_idxs, pred_idxs].mean(), cui_match_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for gt_idx, pred_idx in zip(gt_idxs, pred_idxs):\n",
    "#     if cui_match_scores[gt_idx, pred_idx] != 0:\n",
    "#         print(gt_headings[gt_idx], pred_headings[pred_idx])\n",
    "#         print(\n",
    "#             bert_scorer.score(\n",
    "#                 [gt_bhc.problem_sections[gt_idx].text],\n",
    "#                 [pred_bhc.problem_sections[pred_idx].text],\n",
    "#             )\n",
    "#         )\n",
    "#         print(\n",
    "#             rouge_scorer.score(\n",
    "#                 gt_bhc.problem_sections[gt_idx].text,\n",
    "#                 pred_bhc.problem_sections[pred_idx].text,\n",
    "#             )\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_headings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for pred_idx, gt_idx in zip(pred_idxs, gt_idxs):\n",
    "#     print(pred_headings[gt_idx], gt_headings[pred_idx])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
