{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIMIC Generation\n",
    "\n",
    "This notebooks ingests mimic physician notes and Royal College of Physician London guidelines. \n",
    "\n",
    "These are converted into a prompt and queried to GPT-4-turbo. \n",
    "\n",
    "The simplified json schema used in the prompt is saved to file.\n",
    "\n",
    "The outputs are then saved to `outputs\\llm_responses` dir. This output is the raw json and the message history with the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "from random import Random\n",
    "\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "from llm_discharge_summaries.schemas.mimic import PhysicianNote\n",
    "from llm_discharge_summaries.schemas.rcp_guidelines import RCPGuidelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PHYSICIAN_NOTE_FPATH = (\n",
    "    Path.cwd()\n",
    "    / \"inputs\"\n",
    "    / \"physionet.org\"\n",
    "    / \"files\"\n",
    "    / \"mimiciii\"\n",
    "    / \"1.4\"\n",
    "    / \"physician_notes_mimic.csv\"\n",
    ")\n",
    "ONE_SHOT_EXAMPLE_DIR = (\n",
    "    Path.cwd().parent / \"llm_discharge_summaries\" / \"schemas\" / \"rcp_one_shot_example\"\n",
    ")\n",
    "OUTPUT_DIR = Path.cwd() / \"outputs\" / \"llm_responses\"\n",
    "\n",
    "GPT_4_ENGINE = \"gpt-4-turbo\"\n",
    "AZURE_API_VERSION = \"2023-07-01-preview\"\n",
    "TOKENIZER_NAME = \"cl100k_base\"\n",
    "\n",
    "NUMBER_CLINICAL_EVALUATORS = 15\n",
    "NUM_EXAMPLES_PER_EVALUATOR = 5\n",
    "RANDOM_SEED = 23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcp_schema = RCPGuidelines.schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Title, required fields are removed as contain redundant information. \n",
    "\n",
    "Definition properties with a default value are also removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_keys_recursive(d: list | dict, keys: set[str]):\n",
    "    if isinstance(d, dict):\n",
    "        for key in list(d.keys()):\n",
    "            if key in keys:\n",
    "                del d[key]\n",
    "            else:\n",
    "                remove_keys_recursive(d[key], keys)\n",
    "    elif isinstance(d, list):\n",
    "        for item in d:\n",
    "            remove_keys_recursive(item, keys)\n",
    "    return d\n",
    "\n",
    "\n",
    "def remove_default_definition_properties(schema: dict):\n",
    "    for section_dict in schema[\"definitions\"].values():\n",
    "        section_dict[\"properties\"] = {\n",
    "            property: property_dict\n",
    "            for property, property_dict in section_dict[\"properties\"].items()\n",
    "            if \"default\" not in property_dict.keys()\n",
    "        }\n",
    "    return schema\n",
    "\n",
    "\n",
    "simplified_rcp_schema = remove_keys_recursive(\n",
    "    deepcopy(rcp_schema), {\"title\", \"required\"}\n",
    ")\n",
    "simplified_rcp_schema = remove_default_definition_properties(simplified_rcp_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(OUTPUT_DIR.parent / \"simplified_rcp_schema.json\").write_text(\n",
    "    json.dumps(simplified_rcp_schema, indent=4)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load 1 shot example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_notes = [\n",
    "    PhysicianNote(**note)\n",
    "    for note in json.loads((ONE_SHOT_EXAMPLE_DIR / \"physician_notes.json\").read_text())\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes_df = pd.read_csv(PHYSICIAN_NOTE_FPATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_shared_examples = math.floor(NUMBER_CLINICAL_EVALUATORS / 2)\n",
    "eval_sample_size = (\n",
    "    NUM_EXAMPLES_PER_EVALUATOR * NUMBER_CLINICAL_EVALUATORS - num_shared_examples\n",
    ")\n",
    "eval_sample_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hadm_ids = notes_df[\"HADM_ID\"].unique().tolist()\n",
    "# Used for 1 round of qualitative evaluation\n",
    "sample_hadm_ids = Random(RANDOM_SEED).sample(\n",
    "    hadm_ids, NUM_EXAMPLES_PER_EVALUATOR + eval_sample_size\n",
    ")[NUM_EXAMPLES_PER_EVALUATOR:]\n",
    "train_hadm_ids = sample_hadm_ids[:NUM_EXAMPLES_PER_EVALUATOR]\n",
    "eval_hadm_ids = sample_hadm_ids[NUM_EXAMPLES_PER_EVALUATOR:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "cache_dir = \"/data2/simon/auto-medical-discharge-summaries/.model_cache\"\n",
    "device = \"cuda\"\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "assert torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install flash-attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    cache_dir=cache_dir,\n",
    "    device_map=device,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name, padding_side=\"left\", cache_dir=cache_dir\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_physician_notes = [\n",
    "    PhysicianNote(\n",
    "        hadm_id=row[\"HADM_ID\"],\n",
    "        title=row[\"DESCRIPTION\"],\n",
    "        timestamp=row[\"CHARTTIME\"],\n",
    "        text=row[\"TEXT\"],\n",
    "    )\n",
    "    for _, row in notes_df[notes_df[\"HADM_ID\"] == 103411].iterrows()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f\"\"\"The following document contains examples of the reason for admission extracted from a set of patient clinical notes\n",
    "# This has been done by an expert clinician.\n",
    "\n",
    "# Each example has the following 3 parts:\n",
    "# Patient's Clinical Notes:\n",
    "# [The patient's clinical notes ordered by ascending timestamp. Each note has a title of the format [Title]: [timestamp year-month-day hour:min].]\n",
    "\n",
    "# Reason for admission:\n",
    "# [The main reason why the patient was admitted to hospital, eg chest pain, breathlessness, collapse, etc. This should be symptoms and not the diagnosis]\n",
    "# Examples are separated by\n",
    "# ###\n",
    "# Example 1:\n",
    "# Patient's Clinical Notes:\n",
    "# {_physician_notes_to_string(example_notes)}\n",
    "# Reason for admission:\n",
    "# Chest tightness pain, breathlessness, nausea and dizziness started at 6 am.\n",
    "# ###\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_response = json.loads(\n",
    "    (ONE_SHOT_EXAMPLE_DIR / \"discharge_summary.json\").read_text()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_discharge_summaries.openai_llm.prompts import _physician_notes_to_string\n",
    "from llm_discharge_summaries.schemas.rcp_guidelines import AdmissionDetails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [\n",
    "    (\n",
    "        field_name,\n",
    "        field_schema[\"description\"],\n",
    "        example_response[\"admission_details\"][field_name],\n",
    "    )\n",
    "    for field_name, field_schema in AdmissionDetails.schema()[\"properties\"].items()\n",
    "    if \"default\" not in field_schema.keys()\n",
    "]\n",
    "fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_message_prompt(\n",
    "    field_name: str, description: str, example_response: str | list[str]\n",
    "):\n",
    "    cleaned_field_name = field_name.replace(\"_\", \" \").lower()\n",
    "    example_response_str = (\n",
    "        \", \".join(example_response)\n",
    "        if type(example_response) == list\n",
    "        else example_response\n",
    "    )\n",
    "\n",
    "    return [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"\"\"I am a expert clinician tasked with finding the {cleaned_field_name} for a patient.\n",
    "I will give you a patient's clinical notes from their stay from oldest to most recent. Each note is separated by a blank line and starts with a title followed by a timestamp.\n",
    "Then you must tell me what you the {cleaned_field_name} which is defined as {description.lower()}\n",
    "Responses should be no longer than 40 words.\n",
    "Expand any abbreviations used in the notes to their full medical terms.\n",
    "An example response would be\n",
    "After analyzing the clinical notes I have found that the {cleaned_field_name} is {example_response_str.lower()}\"\"\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": (\n",
    "                \"Of course, I can help with that. Please provide the patient's clinical\"\n",
    "                \" notes.\"\n",
    "            ),\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"\"\"These are the patient's clinical notes \n",
    "{_physician_notes_to_string(train_physician_notes)[:4000]}\"\"\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": (\n",
    "                f\"After analyzing the patient notes the {cleaned_field_name} was\"\n",
    "            ),\n",
    "        },\n",
    "    ]\n",
    "\n",
    "\n",
    "first_prompt_messages = [\n",
    "    first_message_prompt(name, description, example)\n",
    "    for name, description, example in fields\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for first_prompt_message in first_prompt_messages:\n",
    "    print(first_prompt_message[0][\"content\"])\n",
    "    print(\"--\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message_strs = [\n",
    "    tokenizer.apply_chat_template(messages, tokenize=False)[\n",
    "        len(tokenizer.bos_token) : -len(tokenizer.eos_token)\n",
    "    ]\n",
    "    for messages in first_prompt_messages\n",
    "]\n",
    "tokens = tokenizer(message_strs, return_tensors=\"pt\", padding=True).to(device)\n",
    "tokens.input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = GenerationConfig(\n",
    "    max_new_tokens=1000,\n",
    "    do_sample=False,\n",
    "    use_cache=True,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    ")\n",
    "generated_ids = model.generate(**tokens, generation_config=generation_config)\n",
    "output = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doctor_assistant_messages = [\n",
    "    field_output.split(\"[/INST]\")[-1].strip() for field_output in output\n",
    "]\n",
    "doctor_assistant_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def second_message_prompt(\n",
    "    field_name: str,\n",
    "    description: str,\n",
    "    example_response: str | list[str],\n",
    "    doctor_assistant_message: str,\n",
    "):\n",
    "    yaml_string_array_type = \"\"\"array\n",
    "    items:\n",
    "    type: string\"\"\"\n",
    "\n",
    "    yaml_schema = f\"\"\"type: object\n",
    "properties:\n",
    "{field_name}:\n",
    "    description: {description}\n",
    "    type: {\"string\" if type(example_response) == str else yaml_string_array_type}\"\"\"\n",
    "\n",
    "    return [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"\"\"I am an administrator converting doctors notes to a json according to the following yaml schema\n",
    "```yaml\n",
    "{yaml_schema}\n",
    "```\n",
    "\n",
    "Only respond with the schema compliant json object.\n",
    "Expand any abbreviations used by the doctor to their full medical terms.\n",
    "Do not include an explanation.\n",
    "Do not include any additional properties even if additional information is available.\n",
    "An example response would be\n",
    "```json\n",
    "{{'{field_name}': {example_response}}}\n",
    "```\n",
    "\"\"\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": (\n",
    "                \"Of course, I can help with that. Please provide a doctors note.\"\n",
    "            ),\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"\"\"This is the doctor's note\n",
    "{doctor_assistant_message}\"\"\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": f\"\"\"```json\n",
    "{{{field_name}: \"\"\",\n",
    "        },\n",
    "    ]\n",
    "\n",
    "\n",
    "second_message_prompts = [\n",
    "    second_message_prompt(name, description, example, doctor_assistant_message)\n",
    "    for (name, description, example), doctor_assistant_message in zip(\n",
    "        fields, doctor_assistant_messages\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for second_message_prompt in second_message_prompts:\n",
    "    print(second_message_prompt[0][\"content\"])\n",
    "    print(\"--\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message_strs = [\n",
    "    tokenizer.apply_chat_template(messages, tokenize=False)[\n",
    "        len(tokenizer.bos_token) : -len(tokenizer.eos_token)\n",
    "    ]\n",
    "    for messages in second_message_prompts\n",
    "]\n",
    "tokens = tokenizer(message_strs, return_tensors=\"pt\", padding=True).to(device)\n",
    "tokens.input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = GenerationConfig(\n",
    "    max_new_tokens=1000,\n",
    "    do_sample=False,\n",
    "    use_cache=True,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    ")\n",
    "generated_ids = model.generate(**tokens, generation_config=generation_config)\n",
    "output = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_assistant_messages = [\n",
    "    field_output.split(\"[/INST]\")[-1].strip() for field_output in output\n",
    "]\n",
    "schema_assistant_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_between_backticks(text, prefix, suffix):\n",
    "    first_backtick_index = text.find(prefix)\n",
    "    if first_backtick_index == -1:\n",
    "        return None  # If no backticks found\n",
    "    second_backtick_index = text.find(suffix, first_backtick_index + len(prefix))\n",
    "    if second_backtick_index == -1:\n",
    "        return None  # If only one backtick found\n",
    "    return text[first_backtick_index : second_backtick_index + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_assistant_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for message in schema_assistant_messages:\n",
    "    extracted_text = extract_text_between_backticks(message, \"{\", \"}\")\n",
    "    formatted_json_string = (\n",
    "        extracted_text.replace(\"'\", '\"')\n",
    "        .replace(\":\", '\":')\n",
    "        .replace(\",\\n\", ',\\n \"')\n",
    "        .replace(\"{\", '{\"')\n",
    "    )\n",
    "\n",
    "    print(formatted_json_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for first_prompt_message in first_prompt_messages:\n",
    "    print(first_prompt_message[0][\"content\"])\n",
    "    print(\"--\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = output.split(\"[/INST]\")[-1]\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"title\": \"AdmissionDetails\", \"type\": \"string\", \"description\": \"thing\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"AdmissionDetails\": \"thing\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
