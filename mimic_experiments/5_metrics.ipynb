{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "\n",
    "Notebook that produces all metrics presented in results section.\n",
    "\n",
    "Unfortunately, annotated data cannot be publicly released due to MIMIC license restrictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = \"{:,.2f}\".format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = Path.cwd() / \"outputs\"\n",
    "ANNOTATED_DIR = OUTPUT_DIR / \"annotators_eval\"\n",
    "RAW_GENERATED_DIR = OUTPUT_DIR / \"llm_responses\"\n",
    "\n",
    "GPT_4_TURBO_INPUT_COST_PER_1K = 0.01\n",
    "GPT_4_TURBO_OUTPUT_COST_PER_1K = 0.03\n",
    "TOKENIZER_NAME = \"cl100k_base\"\n",
    "\n",
    "ERROR_COLUMNS = [\n",
    "    \"Missed- Severe\",\n",
    "    \"Missed- Minor\",\n",
    "    \"Added- Hallucination\",\n",
    "    \"Added- Not relevant\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load all the eval dfs, needs some pre-processing to get in 'nice' pandas format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dfs = []\n",
    "for annotator_dir in ANNOTATED_DIR.iterdir():\n",
    "    if not annotator_dir.is_dir():\n",
    "        continue\n",
    "\n",
    "    for hadm_id_dir in annotator_dir.iterdir():\n",
    "        if not hadm_id_dir.is_dir():\n",
    "            continue\n",
    "\n",
    "        hadm_id = hadm_id_dir.stem\n",
    "        df = pd.read_excel(\n",
    "            (hadm_id_dir / f\"discharge_summary_{hadm_id}.xlsx\"),\n",
    "            engine=\"openpyxl\",\n",
    "            header=1,\n",
    "        )\n",
    "\n",
    "        # Drop empty rows\n",
    "        df = df.dropna(axis=0, how=\"all\")\n",
    "\n",
    "        # TODO: Remove when fixed in annotator\n",
    "        mask = (df[\"Section\"] == \"Allergies And Adverse Reaction\") & (\n",
    "            df[\"Field\"].isnull()\n",
    "        )\n",
    "        if mask.any():\n",
    "            df.loc[mask, \"Field\"] = \"Causative Agent\"\n",
    "            df.loc[mask, \"Value\"] = \"No known drug allergies or adverse reactions\"\n",
    "            df.loc[-1, \"Field\"] = \"Description Of Reaction\"\n",
    "            df.loc[-1, \"Value\"] = \"No known drug allergies or adverse reactions\"\n",
    "\n",
    "        # Fill empty sections and fields with whatever is above\n",
    "        df[\"Section\"] = df[\"Section\"].ffill()\n",
    "        df[\"Field\"] = df[\"Field\"].ffill()\n",
    "        # Autopopulated fields are not generated by LLM so we can drop them\n",
    "        # from evaluation\n",
    "        df = df.loc[df[\"Value\"] != \"Autopopulated\"]\n",
    "\n",
    "        # Empty cells are not errors so set to 0\n",
    "        df[ERROR_COLUMNS] = df[ERROR_COLUMNS].fillna(0)\n",
    "\n",
    "        # Help with grouping\n",
    "        df[\"Field\"] = df[\"Field\"].str.replace(\n",
    "            r\"Causative Agent [0-9]+\", \"Causative Agent\", regex=True\n",
    "        )\n",
    "        df[\"Field\"] = df[\"Field\"].str.replace(\n",
    "            r\"Description Of Reaction [0-9]+\", \"Description Of Reaction\", regex=True\n",
    "        )\n",
    "\n",
    "        # Help with identification downstream\n",
    "        df.hadm_id = hadm_id\n",
    "        df.annotator = annotator_dir.stem\n",
    "        eval_dfs.append(df)\n",
    "len(eval_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_hadm_ids = {df.hadm_id for df in eval_dfs}\n",
    "len(eval_hadm_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hadm_id_to_discharge_summary = {\n",
    "    directory.stem: json.loads((directory / \"discharge_summary.json\").read_text())\n",
    "    for directory in RAW_GENERATED_DIR.iterdir()\n",
    "    if directory.is_dir() and directory.stem in eval_hadm_ids\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Could have been done nicer but raw messages delimited by *** and in the order\n",
    "- system message\n",
    "- one shot example\n",
    "- user physician notes\n",
    "- llm response\n",
    "- time taken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delimiter = \"\\n\" + \"*\" * 80 + \"\\n\"\n",
    "hadm_id_to_messages = {\n",
    "    directory.stem: list((directory / \"raw_messages.txt\").read_text().split(delimiter))\n",
    "    for directory in RAW_GENERATED_DIR.iterdir()\n",
    "    if directory.is_dir() and directory.stem in eval_hadm_ids\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Completion Metrics\n",
    "### Token Lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(TOKENIZER_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = next(iter(hadm_id_to_messages.values()))\n",
    "# Same for all inputs\n",
    "prompt_token_length = sum(len(tokenizer.encode(message)) for message in messages[:3])\n",
    "prompt_token_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_note_message_length = [\n",
    "    # Message 3 is the physician note message\n",
    "    len(tokenizer.encode(messages[3]))\n",
    "    for messages in hadm_id_to_messages.values()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_summary_message_length = [\n",
    "    # Message 3 is the physician note message\n",
    "    len(tokenizer.encode(messages[4]))\n",
    "    for messages in hadm_id_to_messages.values()\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calc average time and costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_times = [\n",
    "    float(messages[-2].split(\": \")[1]) for messages in hadm_id_to_messages.values()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "costs = []\n",
    "for messages in hadm_id_to_messages.values():\n",
    "    num_input_tokens = sum(len(tokenizer.encode(message)) for message in messages[:4])\n",
    "    num_output_tokens = len(tokenizer.encode(messages[4]))\n",
    "    costs.append(\n",
    "        num_input_tokens / 1000 * GPT_4_TURBO_INPUT_COST_PER_1K\n",
    "        + num_output_tokens / 1000 * GPT_4_TURBO_OUTPUT_COST_PER_1K\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intervals = [25, 50, 75, 100]\n",
    "completion_metrics_df = pd.DataFrame.from_records(\n",
    "    (\n",
    "        (np.percentile(input_note_message_length, intervals)),\n",
    "        (np.percentile(generated_summary_message_length, intervals)),\n",
    "        (np.percentile(inference_times, intervals)),\n",
    "        (np.percentile(costs, intervals)),\n",
    "    ),\n",
    ")\n",
    "completion_metrics_df.index = [\n",
    "    [\n",
    "        \"De-Duplicated Physician Note Length / Tokens\",\n",
    "        \"Output Note Length / Tokens\",\n",
    "        \"Inference Time / secs\",\n",
    "        \"Inference Cost / $\",\n",
    "    ]\n",
    "]\n",
    "completion_metrics_df.columns = [f\"{interval}th Percentile\" for interval in intervals]\n",
    "completion_metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extractive\n",
    "\n",
    "Find % of entries that were extractive i.e. directly from source text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_json_strings(json_object):\n",
    "    strings_list = []\n",
    "\n",
    "    def process_object(obj):\n",
    "        if isinstance(obj, str):\n",
    "            strings_list.append(obj)\n",
    "        elif isinstance(obj, list):\n",
    "            for item in obj:\n",
    "                process_object(item)\n",
    "        elif isinstance(obj, dict):\n",
    "            for value in obj.values():\n",
    "                process_object(value)\n",
    "\n",
    "    process_object(json_object)\n",
    "    return strings_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_extractive_sentences = 0\n",
    "num_generated_sentences = 0\n",
    "\n",
    "for hadm_id in hadm_id_to_messages.keys():\n",
    "    discharge_summary_json = hadm_id_to_discharge_summary[hadm_id]\n",
    "    physician_notes_text_lowercase = hadm_id_to_messages[hadm_id][3].lower()\n",
    "\n",
    "    json_strings = find_json_strings(discharge_summary_json)\n",
    "    json_sentences_lowercase = [\n",
    "        sentence.lower()\n",
    "        for item in json_strings\n",
    "        for sentence in item.split(\". \")\n",
    "        if sentence != \"\"\n",
    "    ]\n",
    "\n",
    "    num_extractive_sentences += sum(\n",
    "        1\n",
    "        for sentence_lowercase in json_sentences_lowercase\n",
    "        if sentence_lowercase in physician_notes_text_lowercase\n",
    "    )\n",
    "    num_generated_sentences += len(json_sentences_lowercase)\n",
    "\n",
    "num_extractive_sentences / num_generated_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose between the 2 to group on a per field or section basis. Variable names assume done on a field basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouping_key = [\"Section\", \"Field\"]\n",
    "# grouping_key = [\"Section\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per dataframe error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_errors_list = []\n",
    "for idx, eval_df in enumerate(eval_dfs):\n",
    "    # Sum num errors per group\n",
    "    eval_df_grouped = eval_df.groupby(grouping_key)\n",
    "    df_field_errors = eval_df_grouped[ERROR_COLUMNS].sum()\n",
    "\n",
    "    # Add column for number of values per group\n",
    "    # Value is counted as either an element in a list or a sentence (tokenized by \". \")\n",
    "    df_field_errors[\"Num Values\"] = [\n",
    "        len(\". \".join(values.values).split(\". \"))\n",
    "        for _, values in eval_df_grouped[\"Value\"]\n",
    "    ]\n",
    "\n",
    "    # Add column for number of values not found per group\n",
    "    not_found_count = (\n",
    "        eval_df[eval_df[\"Value\"] == \"Information not found in notes\"]\n",
    "        .groupby(grouping_key)[\"Value\"]\n",
    "        .count()\n",
    "    )\n",
    "    df_field_errors[\"Not Found\"] = not_found_count\n",
    "    # If no values were found, set to 0\n",
    "    df_field_errors[\"Not Found\"].fillna(0, inplace=True)\n",
    "\n",
    "    # Separate tracking of total errors for median calculation\n",
    "    total_errors_list.append(df_field_errors[ERROR_COLUMNS].sum().sum())\n",
    "\n",
    "    if idx == 0:\n",
    "        field_errors = df_field_errors\n",
    "    else:\n",
    "        field_errors += df_field_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GP Practice is never filled (47 values 47 not found). This is because there are no GPs in America! So drop from analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_errors.drop(\"Gp Practice\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Median number of errors (mean is affected by extreme values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.median(total_errors_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calc per field metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_false_positives = (\n",
    "    field_errors[\"Added- Hallucination\"] + field_errors[\"Added- Not relevant\"]\n",
    ")\n",
    "field_false_negatives = field_errors[\"Missed- Severe\"] + field_errors[\"Missed- Minor\"]\n",
    "field_true_positives = field_errors[\"Num Values\"] - field_false_positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_recall = field_true_positives / (field_true_positives + field_false_negatives)\n",
    "field_precision = field_true_positives / (field_true_positives + field_false_positives)\n",
    "field_f1 = 2 * (field_precision * field_recall) / (field_precision + field_recall)\n",
    "field_accuracy = field_true_positives / (\n",
    "    field_true_positives + field_false_negatives + field_false_positives\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calc mean num elements and not found elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_mean_num_elements = field_errors[\"Num Values\"] / len(eval_dfs)\n",
    "field_mean_not_found = field_errors[\"Not Found\"] / len(eval_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics = pd.concat(\n",
    "    [\n",
    "        field_mean_num_elements,\n",
    "        field_mean_not_found,\n",
    "        field_recall,\n",
    "        field_precision,\n",
    "        field_f1,\n",
    "        field_accuracy,\n",
    "    ],\n",
    "    keys=[\n",
    "        \"Average Number of Elements\",\n",
    "        \"Proportion Not Found in Notes\",\n",
    "        \"Recall\",\n",
    "        \"Precision\",\n",
    "        \"F1\",\n",
    "        \"Accuracy\",\n",
    "    ],\n",
    "    axis=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat same calculations but summed across all evaluations to give micro averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_errors = field_errors.sum()\n",
    "total_num_elements = total_errors[\"Num Values\"] / len(eval_dfs)\n",
    "total_not_found = total_errors[\"Not Found\"] / len(eval_dfs)\n",
    "total_false_positives = (\n",
    "    total_errors[\"Added- Hallucination\"] + total_errors[\"Added- Not relevant\"]\n",
    ")\n",
    "total_false_negatives = total_errors[\"Missed- Severe\"] + total_errors[\"Missed- Minor\"]\n",
    "total_true_positives = total_errors[\"Num Values\"] - total_false_positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "micro_recall = total_true_positives / (total_true_positives + total_false_negatives)\n",
    "micro_precision = total_true_positives / (total_true_positives + total_false_positives)\n",
    "micro_f1 = 2 * (micro_precision * micro_recall) / (micro_precision + micro_recall)\n",
    "micro_accuracy = total_true_positives / (\n",
    "    total_true_positives + total_false_negatives + total_false_positives\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_proportions = (\n",
    "    total_errors[ERROR_COLUMNS] / total_errors[ERROR_COLUMNS].sum() * 100\n",
    ")\n",
    "error_proportions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_metrics.mean(axis=0))\n",
    "print([micro_recall, micro_precision, micro_f1, micro_accuracy])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inter annotator agreement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get paired annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hadm_id_to_annotator_dfs = defaultdict(list)\n",
    "for eval_df in eval_dfs:\n",
    "    hadm_id_to_annotator_dfs[eval_df.hadm_id].append(eval_df)\n",
    "\n",
    "paired_hadm_id_to_annotator_dfs = {\n",
    "    hadm_id: dfs for hadm_id, dfs in hadm_id_to_annotator_dfs.items() if len(dfs) == 2\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each annotator is a 4-d vector of the number of errors of each type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_annotated_fields: list[tuple[tuple, tuple]] = []\n",
    "for hadm_id, annotator_dfs in paired_hadm_id_to_annotator_dfs.items():\n",
    "    annotator_1_fields = (\n",
    "        annotator_dfs[0]\n",
    "        .groupby(grouping_key)\n",
    "        .sum()[ERROR_COLUMNS]\n",
    "        .values.astype(int)\n",
    "        .tolist()\n",
    "    )\n",
    "    annotator_2_fields = (\n",
    "        annotator_dfs[1]\n",
    "        .groupby(grouping_key)\n",
    "        .sum()[ERROR_COLUMNS]\n",
    "        .values.astype(int)\n",
    "        .tolist()\n",
    "    )\n",
    "\n",
    "    multi_annotated_fields.extend(\n",
    "        [\n",
    "            (tuple(annotator_1_field), tuple(annotator_2_field))\n",
    "            for annotator_1_field, annotator_2_field in zip(\n",
    "                annotator_1_fields, annotator_2_fields\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "len(multi_annotated_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_and_fp_counts = [\n",
    "    ((sum(anno_0[:2]), sum(anno_0[2:])), (sum(anno_1[:2]), sum(anno_1[2:])))\n",
    "    for anno_0, anno_1 in multi_annotated_fields\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_and_fp_agreement = sum(1 for y_0, y_1 in fn_and_fp_counts if y_0 == y_1) / len(\n",
    "    fn_and_fp_counts\n",
    ")\n",
    "fn_and_fp_agreement"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
