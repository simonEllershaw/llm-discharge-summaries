{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "\n",
    "Notebook that produces all metrics presented in results section.\n",
    "\n",
    "Unfortunately, annotated data cannot be publicly released due to MIMIC license restrictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import Counter, defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "\n",
    "from llm_discharge_summaries.metrics.distance_functions import (\n",
    "    delta_distance,\n",
    "    is_error,\n",
    "    l1_distance,\n",
    ")\n",
    "from llm_discharge_summaries.metrics.krippendorffs_alpha import calc_krippendorffs_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = \"{:,.2f}\".format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = Path.cwd() / \"outputs\"\n",
    "ANNOTATED_DIR = OUTPUT_DIR / \"annotators_eval\"\n",
    "RAW_GENERATED_DIR = OUTPUT_DIR / \"llm_responses\"\n",
    "\n",
    "GPT_4_TURBO_INPUT_COST_PER_1K = 0.01\n",
    "GPT_4_TURBO_OUTPUT_COST_PER_1K = 0.03\n",
    "TOKENIZER_NAME = \"cl100k_base\"\n",
    "\n",
    "ERROR_COLUMNS = [\n",
    "    \"Missed- Severe\",\n",
    "    \"Missed- Minor\",\n",
    "    \"Added- Hallucination\",\n",
    "    \"Added- Not relevant\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hadm_id_to_discharge_summary = {\n",
    "    directory.stem: json.loads((directory / \"discharge_summary.json\").read_text())\n",
    "    for directory in RAW_GENERATED_DIR.iterdir()\n",
    "    if directory.is_dir()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Could have been done nicer but raw messages delimited by *** and in the order\n",
    "- system message\n",
    "- one shot example\n",
    "- user physician notes\n",
    "- llm response\n",
    "- time taken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delimiter = \"\\n\" + \"*\" * 80 + \"\\n\"\n",
    "hadm_id_to_messages = {\n",
    "    directory.stem: list((directory / \"raw_messages.txt\").read_text().split(delimiter))\n",
    "    for directory in RAW_GENERATED_DIR.iterdir()\n",
    "    if directory.is_dir()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extractive\n",
    "\n",
    "Find % of entries that were extractive i.e. directly from source text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_json_strings(json_object):\n",
    "    strings_list = []\n",
    "\n",
    "    def process_object(obj):\n",
    "        if isinstance(obj, str):\n",
    "            strings_list.append(obj)\n",
    "        elif isinstance(obj, list):\n",
    "            for item in obj:\n",
    "                process_object(item)\n",
    "        elif isinstance(obj, dict):\n",
    "            for value in obj.values():\n",
    "                process_object(value)\n",
    "\n",
    "    process_object(json_object)\n",
    "    return strings_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_extractive_sentences = 0\n",
    "num_generated_sentences = 0\n",
    "\n",
    "for hadm_id in hadm_id_to_messages.keys():\n",
    "    discharge_summary_json = hadm_id_to_discharge_summary[hadm_id]\n",
    "    physician_notes_text_lowercase = hadm_id_to_messages[hadm_id][3].lower()\n",
    "\n",
    "    json_strings = find_json_strings(discharge_summary_json)\n",
    "    json_sentences_lowercase = [\n",
    "        sentence.lower()\n",
    "        for item in json_strings\n",
    "        for sentence in item.split(\". \")\n",
    "        if sentence != \"\"\n",
    "    ]\n",
    "\n",
    "    num_extractive_sentences += sum(\n",
    "        1\n",
    "        for sentence_lowercase in json_sentences_lowercase\n",
    "        if sentence_lowercase in physician_notes_text_lowercase\n",
    "    )\n",
    "    num_generated_sentences += len(json_sentences_lowercase)\n",
    "\n",
    "num_extractive_sentences / num_generated_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token Lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(TOKENIZER_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = next(iter(hadm_id_to_messages.values()))\n",
    "# Same for all inputs\n",
    "prompt_token_length = sum(len(tokenizer.encode(message)) for message in messages[:3])\n",
    "prompt_token_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "note_message_length_token = []\n",
    "note_message_length_char = []\n",
    "for messages in hadm_id_to_messages.values():\n",
    "    # Message 3 is the physician note message\n",
    "    note_message = messages[3]\n",
    "    note_message_length_char.append(len(note_message))\n",
    "    note_message_length_token.append(len(tokenizer.encode(note_message)))\n",
    "\n",
    "print(\n",
    "    np.percentile(note_message_length_char, [25, 50, 75]),\n",
    "    np.max(note_message_length_char),\n",
    ")\n",
    "print(\n",
    "    np.percentile(note_message_length_token, [25, 50, 75]),\n",
    "    np.max(note_message_length_token),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Completion metrics\n",
    "\n",
    "Calc average time and costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion_times = [\n",
    "    float(messages[-2].split(\": \")[1]) for messages in hadm_id_to_messages.values()\n",
    "]\n",
    "\n",
    "print(np.percentile(completion_times, [25, 50, 75]), np.max(completion_times))\n",
    "print(np.percentile(completion_times, [25, 50, 75]), np.max(completion_times))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "costs = []\n",
    "for messages in hadm_id_to_messages.values():\n",
    "    num_input_tokens = sum(len(tokenizer.encode(message)) for message in messages[:4])\n",
    "    num_output_tokens = len(tokenizer.encode(messages[4]))\n",
    "    costs.append(\n",
    "        num_input_tokens / 1000 * GPT_4_TURBO_INPUT_COST_PER_1K\n",
    "        + num_output_tokens / 1000 * GPT_4_TURBO_OUTPUT_COST_PER_1K\n",
    "    )\n",
    "print(np.percentile(costs, [25, 50, 75]), np.max(costs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load all the eval dfs, needs some pre-processing to get in 'nice' pandas format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dfs = []\n",
    "for annotator_dir in ANNOTATED_DIR.iterdir():\n",
    "    if not annotator_dir.is_dir():\n",
    "        continue\n",
    "    for hadm_id_dir in annotator_dir.iterdir():\n",
    "        if not hadm_id_dir.is_dir():\n",
    "            continue\n",
    "        hadm_id = hadm_id_dir.stem\n",
    "        df = pd.read_excel(\n",
    "            (hadm_id_dir / f\"discharge_summary_{hadm_id}.xlsx\"),\n",
    "            engine=\"openpyxl\",\n",
    "            header=1,\n",
    "        )\n",
    "        # Drop empty rows\n",
    "        df = df.dropna(axis=0, how=\"all\")\n",
    "\n",
    "        # TODO: Remove when fixed in annotator\n",
    "        mask = (df[\"Section\"] == \"Allergies And Adverse Reaction\") & (\n",
    "            df[\"Field\"].isnull()\n",
    "        )\n",
    "        if mask.any():\n",
    "            df.loc[mask, \"Field\"] = \"Causative Agent\"\n",
    "            df.loc[mask, \"Value\"] = \"No known drug allergies or adverse reactions\"\n",
    "\n",
    "        # Fill empty sections and fields with whatever is above\n",
    "        df[\"Section\"] = df[\"Section\"].ffill()\n",
    "        df[\"Field\"] = df[\"Field\"].ffill()\n",
    "        # Autopopulated fields are not generated by LLM so we can drop them\n",
    "        # from evaluation\n",
    "        df = df.loc[df[\"Value\"] != \"Autopopulated\"]\n",
    "\n",
    "        # Empty cells are not errors so set to 0\n",
    "        df[\n",
    "            [\n",
    "                \"Missed- Severe\",\n",
    "                \"Missed- Minor\",\n",
    "                \"Added- Hallucination\",\n",
    "                \"Added- Not relevant\",\n",
    "            ]\n",
    "        ] = df[\n",
    "            [\n",
    "                \"Missed- Severe\",\n",
    "                \"Missed- Minor\",\n",
    "                \"Added- Hallucination\",\n",
    "                \"Added- Not relevant\",\n",
    "            ]\n",
    "        ].fillna(\n",
    "            0\n",
    "        )\n",
    "\n",
    "        # Help with grouping\n",
    "        df[\"Field\"] = df[\"Field\"].str.replace(\n",
    "            r\"Causative Agent [0-9]+\", \"Causative Agent\", regex=True\n",
    "        )\n",
    "        df[\"Field\"] = df[\"Field\"].str.replace(\n",
    "            r\"Description Of Reaction [0-9]+\", \"Description Of Reaction\", regex=True\n",
    "        )\n",
    "\n",
    "        # Help with identification downstream\n",
    "        df.hadm_id = hadm_id\n",
    "        df.annotator = annotator_dir.stem\n",
    "        eval_dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dfs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose between the 2 to group on a per field or section basis. Variable names assume done on a field basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouping_key = [\"Section\", \"Field\"]\n",
    "# grouping_key = [\"Section\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per dataframe error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_errors_list = []\n",
    "for idx, eval_df in enumerate(eval_dfs):\n",
    "    # Sum num errors per group\n",
    "    df_field_errors = eval_df.groupby(grouping_key)[ERROR_COLUMNS].sum()\n",
    "    # Add column for number of values per group\n",
    "    df_field_errors[\"Num Values\"] = eval_df.groupby(grouping_key)[\"Value\"].count()\n",
    "\n",
    "    # Add column for number of values not found per group\n",
    "    not_found_count = (\n",
    "        eval_df[eval_df[\"Value\"] == \"Information not found in notes\"]\n",
    "        .groupby(grouping_key)[\"Value\"]\n",
    "        .count()\n",
    "    )\n",
    "    df_field_errors[\"Not Found\"] = not_found_count\n",
    "    # If no values were found, set to 0\n",
    "    df_field_errors[\"Not Found\"].fillna(0, inplace=True)\n",
    "\n",
    "    # Clinical summary is a free text paragraph, so we estimate each sentence as a value\n",
    "    clinical_summary_text = eval_df[\n",
    "        (eval_df[\"Section\"] == \"Clinical Summary\")\n",
    "        & (eval_df[\"Field\"] == \"Clinical Summary\")\n",
    "    ][\"Value\"].iloc[0]\n",
    "    estimated_num_sentences = len(clinical_summary_text.split(\". \"))\n",
    "    df_field_errors.at[(\"Clinical Summary\", \"Clinical Summary\"), \"Num Values\"] = (\n",
    "        estimated_num_sentences\n",
    "    )\n",
    "\n",
    "    # Separate tracking of total errors for micro calculation later\n",
    "    total_errors_list.append(df_field_errors[ERROR_COLUMNS].sum().sum())\n",
    "\n",
    "    if idx == 0:\n",
    "        field_errors = df_field_errors\n",
    "    else:\n",
    "        field_errors += df_field_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GP Practice is never filled (47 values 47 not found). This is because there are no GPs in America! So drop from analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_errors.drop(\"Gp Practice\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Median number of errors (mean is affected by extreme values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.median(total_errors_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calc per field metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_false_positives = (\n",
    "    field_errors[\"Added- Hallucination\"] + field_errors[\"Added- Not relevant\"]\n",
    ")\n",
    "field_false_negatives = field_errors[\"Missed- Severe\"] + field_errors[\"Missed- Minor\"]\n",
    "field_true_positives = field_errors[\"Num Values\"] - field_false_positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_recall = field_true_positives / (field_true_positives + field_false_negatives)\n",
    "field_precision = field_true_positives / (field_true_positives + field_false_positives)\n",
    "field_f1 = 2 * (field_precision * field_recall) / (field_precision + field_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calc mean num elements and not found elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_mean_num_elements = field_errors[\"Num Values\"] / len(eval_dfs)\n",
    "field_mean_not_found = field_errors[\"Not Found\"] / len(eval_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics = pd.concat(\n",
    "    [\n",
    "        field_mean_num_elements,\n",
    "        field_mean_not_found,\n",
    "        field_recall,\n",
    "        field_precision,\n",
    "        field_f1,\n",
    "    ],\n",
    "    keys=[\n",
    "        \"Average Number of Elements\",\n",
    "        \"Proportion Not Found in Notes\",\n",
    "        \"Recall\",\n",
    "        \"Precision\",\n",
    "        \"F1\",\n",
    "    ],\n",
    "    axis=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat same calculations but summed across all evaluations to give micro averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_errors = field_errors.sum()\n",
    "total_num_elements = total_errors[\"Num Values\"] / len(eval_dfs)\n",
    "total_not_found = total_errors[\"Not Found\"] / len(eval_dfs)\n",
    "total_false_positives = (\n",
    "    total_errors[\"Added- Hallucination\"] + total_errors[\"Added- Not relevant\"]\n",
    ")\n",
    "total_false_negatives = total_errors[\"Missed- Severe\"] + total_errors[\"Missed- Minor\"]\n",
    "total_true_positives = total_errors[\"Num Values\"] - total_false_positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "micro_recall = total_true_positives / (total_true_positives + total_false_negatives)\n",
    "micro_precision = total_true_positives / (total_true_positives + total_false_positives)\n",
    "micro_f1 = 2 * (micro_precision * micro_recall) / (micro_precision + micro_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_metrics.mean(axis=0))\n",
    "print([micro_recall, micro_precision, micro_f1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inter annotator agreement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get paired annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hadm_id_to_annotator_dfs = defaultdict(list)\n",
    "for eval_df in eval_dfs:\n",
    "    hadm_id_to_annotator_dfs[eval_df.hadm_id].append(eval_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each annotator is a 4-d vector of the number of errors of each type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_annotated_fields: list[list[tuple[int]]] = []\n",
    "for hadm_id, annotator_dfs in hadm_id_to_annotator_dfs.items():\n",
    "    if len(annotator_dfs) == 2:\n",
    "        annotator_1_fields = (\n",
    "            eval_dfs[0]\n",
    "            .groupby(grouping_key)\n",
    "            .sum()[ERROR_COLUMNS]\n",
    "            .values.astype(int)\n",
    "            .tolist()\n",
    "        )\n",
    "        annotator_2_fields = (\n",
    "            eval_dfs[1]\n",
    "            .groupby(grouping_key)\n",
    "            .sum()[ERROR_COLUMNS]\n",
    "            .values.astype(int)\n",
    "            .tolist()\n",
    "        )\n",
    "        multi_annotated_fields.extend(\n",
    "            [\n",
    "                # Mypy (fairly) can't infer that the elements are ints\n",
    "                [tuple(annotator_1_field), tuple(annotator_2_field)]  # type: ignore\n",
    "                for annotator_1_field, annotator_2_field in zip(\n",
    "                    annotator_1_fields, annotator_2_fields\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "len(multi_annotated_fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calc proportion of annotations that agree if field has error Y/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 - sum(\n",
    "    is_error(multi_annotated_field[0], multi_annotated_field[1])\n",
    "    for multi_annotated_field in multi_annotated_fields\n",
    ") / len(multi_annotated_fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calc proportion of annotations that exactly match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 - sum(\n",
    "    delta_distance(multi_annotated_field[0], multi_annotated_field[1])\n",
    "    for multi_annotated_field in multi_annotated_fields\n",
    ") / len(multi_annotated_fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calc krippendorffs_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_krippendorffs_alpha(multi_annotated_fields, delta_distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization of distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observed_distances = [\n",
    "    l1_distance(multi_annotated_field[0], multi_annotated_field[1])\n",
    "    for multi_annotated_field in multi_annotated_fields\n",
    "]\n",
    "\n",
    "all_annotations = []\n",
    "\n",
    "all_annotations = [\n",
    "    annotation\n",
    "    for multi_annotated_field in multi_annotated_fields\n",
    "    for annotation in multi_annotated_field\n",
    "]\n",
    "expected_distances = [\n",
    "    l1_distance(annotation_1, annotation_2)\n",
    "    for annotation_1 in all_annotations\n",
    "    for annotation_2 in all_annotations\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot paired distances and inter-items distances in a histogram normalized to 1\n",
    "plt.hist(observed_distances, alpha=0.5, label=\"observed\", density=True)\n",
    "plt.hist(expected_distances, alpha=0.5, label=\"expected\", density=True)\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(all_annotations)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
