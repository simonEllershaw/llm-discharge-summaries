{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVAL_DIR = Path.cwd() / \"output_eval_v3\"\n",
    "GPT_4_TURBO_INPUT_COST_PER_1K = 0.01\n",
    "GPT_4_TURBO_OUTPUT_COST_PER_1K = 0.03\n",
    "TOKENIZER_NAME = \"cl100k_base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_json_strings(json_object):\n",
    "    strings_list = []\n",
    "\n",
    "    def process_object(obj):\n",
    "        if isinstance(obj, str):\n",
    "            strings_list.append(obj)\n",
    "        elif isinstance(obj, list):\n",
    "            for item in obj:\n",
    "                process_object(item)\n",
    "        elif isinstance(obj, dict):\n",
    "            for value in obj.values():\n",
    "                process_object(value)\n",
    "\n",
    "    process_object(json_object)\n",
    "    return strings_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_extractive_sentences = 0\n",
    "num_generated_sentences = 0\n",
    "\n",
    "for dir in EVAL_DIR.iterdir():\n",
    "    discharge_summary_json = json.loads((dir / \"discharge_summary.json\").read_text())\n",
    "\n",
    "    raw_message_text = (dir / \"raw_messages.txt\").read_text()\n",
    "    raw_message_sections = raw_message_text.split(\"\\n\" + \"*\" * 80 + \"\\n\")\n",
    "    physician_notes_text_lowercase = raw_message_sections[3].lower()\n",
    "\n",
    "    json_strings = find_json_strings(discharge_summary_json)\n",
    "    json_sentences_lowercase = [\n",
    "        sentence.lower()\n",
    "        for item in json_strings\n",
    "        for sentence in item.split(\". \")\n",
    "        if sentence != \"\"\n",
    "    ]\n",
    "\n",
    "    num_extractive_sentences += sum(\n",
    "        1\n",
    "        for sentence_lowercase in json_sentences_lowercase\n",
    "        if sentence_lowercase in physician_notes_text_lowercase\n",
    "    )\n",
    "    num_generated_sentences += len(json_sentences_lowercase)\n",
    "\n",
    "num_extractive_sentences, num_generated_sentences, num_extractive_sentences / num_generated_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discharge_summary_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(TOKENIZER_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_taken = []\n",
    "input_token_lengths = []\n",
    "output_token_lengths = []\n",
    "\n",
    "for dir in EVAL_DIR.iterdir():\n",
    "    raw_message_text = (dir / \"raw_messages.txt\").read_text()\n",
    "    raw_message_sections = raw_message_text.split(\"\\n\" + \"*\" * 80 + \"\\n\")\n",
    "\n",
    "    input_token_lengths.append(\n",
    "        sum(len(tokenizer.encode(message)) for message in raw_message_sections[:4])\n",
    "    )\n",
    "    output_token_lengths.append(len(tokenizer.encode(raw_message_sections[4])))\n",
    "\n",
    "    time_taken.append(float(raw_message_sections[-2].split(\": \")[1]))\n",
    "\n",
    "time_taken_np = np.array(time_taken)\n",
    "input_token_lengths_np = np.array(input_token_lengths)\n",
    "output_token_lengths_np = np.array(output_token_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    time_taken_np.mean(), time_taken_np.std(), time_taken_np.min(), time_taken_np.max()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    input_token_lengths_np.mean(),\n",
    "    input_token_lengths_np.std(),\n",
    "    input_token_lengths_np.min(),\n",
    "    input_token_lengths_np.max(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    output_token_lengths_np.mean(),\n",
    "    output_token_lengths_np.std(),\n",
    "    output_token_lengths_np.min(),\n",
    "    output_token_lengths_np.max(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = (input_token_lengths_np / 1000 * GPT_4_TURBO_INPUT_COST_PER_1K) + (\n",
    "    output_token_lengths_np / 1000 * GPT_4_TURBO_OUTPUT_COST_PER_1K\n",
    ")\n",
    "print(cost.mean(), cost.std(), cost.min(), cost.max())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
