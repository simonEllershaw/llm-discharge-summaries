{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANNOTATED_DIR = Path.cwd() / \"outputs\" / \"annotated\"\n",
    "RAW_GENERATED_DIR = Path.cwd() / \"outputs\" / \"output_eval_v3\"\n",
    "GPT_4_TURBO_INPUT_COST_PER_1K = 0.01\n",
    "GPT_4_TURBO_OUTPUT_COST_PER_1K = 0.03\n",
    "TOKENIZER_NAME = \"cl100k_base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hadm_id_to_discharge_summary = {\n",
    "    directory.stem: json.loads((directory / \"discharge_summary.json\").read_text())\n",
    "    for directory in RAW_GENERATED_DIR.iterdir()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delimter = \"\\n\" + \"*\" * 80 + \"\\n\"\n",
    "hadm_id_to_messages = {\n",
    "    directory.stem: list((directory / \"raw_messages.txt\").read_text().split(delimter))\n",
    "    for directory in RAW_GENERATED_DIR.iterdir()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extractive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_json_strings(json_object):\n",
    "    strings_list = []\n",
    "\n",
    "    def process_object(obj):\n",
    "        if isinstance(obj, str):\n",
    "            strings_list.append(obj)\n",
    "        elif isinstance(obj, list):\n",
    "            for item in obj:\n",
    "                process_object(item)\n",
    "        elif isinstance(obj, dict):\n",
    "            for value in obj.values():\n",
    "                process_object(value)\n",
    "\n",
    "    process_object(json_object)\n",
    "    return strings_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_extractive_sentences = 0\n",
    "num_generated_sentences = 0\n",
    "\n",
    "for hadm_id in hadm_id_to_messages.keys():\n",
    "    discharge_summary_json = hadm_id_to_discharge_summary[hadm_id]\n",
    "    physician_notes_text_lowercase = hadm_id_to_messages[hadm_id][3].lower()\n",
    "\n",
    "    json_strings = find_json_strings(discharge_summary_json)\n",
    "    json_sentences_lowercase = [\n",
    "        sentence.lower()\n",
    "        for item in json_strings\n",
    "        for sentence in item.split(\". \")\n",
    "        if sentence != \"\"\n",
    "    ]\n",
    "\n",
    "    num_extractive_sentences += sum(\n",
    "        1\n",
    "        for sentence_lowercase in json_sentences_lowercase\n",
    "        if sentence_lowercase in physician_notes_text_lowercase\n",
    "    )\n",
    "    num_generated_sentences += len(json_sentences_lowercase)\n",
    "\n",
    "num_extractive_sentences / num_generated_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(TOKENIZER_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = next(iter(hadm_id_to_messages.values()))\n",
    "prompt_token_length = sum(len(tokenizer.encode(message)) for message in messages[:3])\n",
    "prompt_token_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "note_message_length_token = []\n",
    "note_message_length_char = []\n",
    "for messages in hadm_id_to_messages.values():\n",
    "    note_message = messages[3]\n",
    "    note_message_length_char.append(len(note_message))\n",
    "    note_message_length_token.append(len(tokenizer.encode(note_message)))\n",
    "\n",
    "print(\n",
    "    np.percentile(note_message_length_char, [25, 50, 75]),\n",
    "    np.max(note_message_length_char),\n",
    ")\n",
    "print(\n",
    "    np.percentile(note_message_length_token, [25, 50, 75]),\n",
    "    np.max(note_message_length_token),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Completion metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion_times = [\n",
    "    float(messages[-2].split(\": \")[1]) for messages in hadm_id_to_messages.values()\n",
    "]\n",
    "\n",
    "print(np.percentile(completion_times, [25, 50, 75]), np.max(completion_times))\n",
    "print(np.percentile(completion_times, [25, 50, 75]), np.max(completion_times))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "costs = []\n",
    "for messages in hadm_id_to_messages.values():\n",
    "    num_input_tokens = sum(len(tokenizer.encode(message)) for message in messages[:4])\n",
    "    num_output_tokens = len(tokenizer.encode(messages[4]))\n",
    "    costs.append(\n",
    "        num_input_tokens / 1000 * GPT_4_TURBO_INPUT_COST_PER_1K\n",
    "        + num_output_tokens / 1000 * GPT_4_TURBO_OUTPUT_COST_PER_1K\n",
    "    )\n",
    "print(np.percentile(costs, [25, 50, 75]), np.max(costs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dfs = []\n",
    "for annotator_dir in ANNOTATED_DIR.iterdir():\n",
    "    for hadm_id_dir in annotator_dir.iterdir():\n",
    "        hadm_id = hadm_id_dir.stem\n",
    "        df = pd.read_excel(\n",
    "            (hadm_id_dir / f\"discharge_summary_{hadm_id}.xlsx\"),\n",
    "            engine=\"openpyxl\",\n",
    "            header=1,\n",
    "        )\n",
    "\n",
    "        df.drop(columns=[\"Unnamed: 3\"], inplace=True)\n",
    "        df = df.dropna(axis=0, how=\"all\")\n",
    "        df[\"Section\"] = df[\"Section\"].fillna(method=\"ffill\")\n",
    "        df[\"Field\"] = df[\"Field\"].fillna(method=\"ffill\")\n",
    "        df = df.loc[df[\"Value\"] != \"Autopopulated\"]\n",
    "        df[\n",
    "            [\n",
    "                \"Missed- Severe\",\n",
    "                \"Missed- Minor\",\n",
    "                \"Added- Hallucination\",\n",
    "                \"Added- Not relevant\",\n",
    "            ]\n",
    "        ] = df[\n",
    "            [\n",
    "                \"Missed- Severe\",\n",
    "                \"Missed- Minor\",\n",
    "                \"Added- Hallucination\",\n",
    "                \"Added- Not relevant\",\n",
    "            ]\n",
    "        ].fillna(\n",
    "            0\n",
    "        )\n",
    "        df[\"Field\"] = df[\"Field\"].str.replace(\n",
    "            r\"Causative Agent [0-9]+\", \"Causative Agent\", regex=True\n",
    "        )\n",
    "        df[\"Field\"] = df[\"Field\"].str.replace(\n",
    "            r\"Description Of Reaction [0-9]+\", \"Description Of Reaction\", regex=True\n",
    "        )\n",
    "        eval_dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_types = [\n",
    "    \"Missed- Severe\",\n",
    "    \"Missed- Minor\",\n",
    "    \"Added- Hallucination\",\n",
    "    \"Added- Not relevant\",\n",
    "]\n",
    "for idx, eval_df in enumerate(eval_dfs):\n",
    "    df_section_errors = eval_df.groupby([\"Section\"])[error_types].sum()\n",
    "    df_section_errors[\"Num Values\"] = eval_df.groupby([\"Section\"])[\"Value\"].count()\n",
    "\n",
    "    # Clinical summary is a free text paragraph, so we estimate each sentnece as a value\n",
    "    clinical_summary_text = eval_df[\n",
    "        (eval_df[\"Section\"] == \"Clinical Summary\")\n",
    "        & (eval_df[\"Field\"] == \"Clinical Summary\")\n",
    "    ][\"Value\"].iloc[0]\n",
    "    estimated_num_sentences = len(clinical_summary_text.split(\". \"))\n",
    "    # -1 as already counted once\n",
    "    df_section_errors.loc[\"Clinical Summary\", \"Num Values\"] += (\n",
    "        estimated_num_sentences - 1\n",
    "    )\n",
    "\n",
    "    if idx == 0:\n",
    "        section_errors = df_section_errors\n",
    "    else:\n",
    "        section_errors += df_section_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "section_false_positives = (\n",
    "    section_errors[\"Added- Hallucination\"] + section_errors[\"Added- Not relevant\"]\n",
    ")\n",
    "section_false_negatives = (\n",
    "    section_errors[\"Missed- Severe\"] + section_errors[\"Missed- Minor\"]\n",
    ")\n",
    "section_true_positives = section_errors[\"Num Values\"] - section_false_positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "section_recall = section_true_positives / (\n",
    "    section_true_positives + section_false_negatives\n",
    ")\n",
    "section_precision = section_true_positives / (\n",
    "    section_true_positives + section_false_positives\n",
    ")\n",
    "section_f1 = (\n",
    "    2 * (section_precision * section_recall) / (section_precision + section_recall)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "section_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_errors = section_errors.sum()\n",
    "total_false_positives = (\n",
    "    total_errors[\"Added- Hallucination\"] + total_errors[\"Added- Not relevant\"]\n",
    ")\n",
    "total_false_negatives = total_errors[\"Missed- Severe\"] + total_errors[\"Missed- Minor\"]\n",
    "total_true_positives = total_errors[\"Num Values\"] - total_false_positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "micro_recall = total_true_positives / (total_true_positives + total_false_negatives)\n",
    "micro_precision = total_true_positives / (total_true_positives + total_false_positives)\n",
    "micro_f1 = 2 * (micro_precision * micro_recall) / (micro_precision + micro_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics = pd.concat(\n",
    "    [section_recall, section_precision, section_f1],\n",
    "    keys=[\"Recall\", \"Precision\", \"F1\"],\n",
    "    axis=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics.loc[\"Macro\"] = df_metrics.mean(axis=0)\n",
    "df_metrics.loc[\"Micro\"] = [micro_recall, micro_precision, micro_f1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
