{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import re\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "from spacy.lang.en import English\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from discharge_summaries.preprocessing.preprocess_snomed import Snomed\n",
    "from discharge_summaries.schemas.mimic import Record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path.cwd().parent / \"data\"\n",
    "\n",
    "TRAINING_DATASET_PATH = DATA_DIR / \"train_all_ds.pkl\"\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y_%m_%d_%H_%M\")\n",
    "TRAINING_ANNO_DATASET_PATH = DATA_DIR / f\"train_anno_all_{TIMESTAMP}.pkl\"\n",
    "RANDOM_SEED = 23\n",
    "SNOMED_PATH = (\n",
    "    Path.cwd().parent / \"data\" / \"SnomedCT_InternationalRF2_PRODUCTION_20230731T120000Z\"\n",
    ")\n",
    "\n",
    "SPACY_MODEL = \"en_core_sci_md\"\n",
    "MAX_SEGMENT_TOKEN_LENGTH = 128\n",
    "HF_MODEL_NAME = \"roberta-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(TRAINING_DATASET_PATH, \"rb\") as in_file:\n",
    "    dataset = [Record(**record) for record in pickle.load(in_file)]\n",
    "dataset = dataset\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing SNOMED CT for MedCAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sowmed = Snomed(str(SNOMED_PATH))\n",
    "sowmed.uk_ext = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sowmed.to_concept_df()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"description_type_ids\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_type_names = {\n",
    "    \"disorder\",\n",
    "    \"finding\",\n",
    "    \"morphologic abnormality\",\n",
    "    \"organism\",\n",
    "    \"physical object\",\n",
    "    \"clinical drug\",\n",
    "    \"medicinal product form\",\n",
    "    \"procedure\",\n",
    "    \"product\",\n",
    "}\n",
    "assert all(name in df[\"description_type_ids\"].unique() for name in filter_type_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[\"description_type_ids\"].isin(filter_type_names)]\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_subset = df[df['description_type_ids'].isin(['finding', 'disorder'])]\n",
    "df_subset = df[df[\"name_status\"] == \"A\"]\n",
    "len(df_subset), len(df_subset[\"cui\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_spacy = English().tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snomed_matcher = PhraseMatcher(tokenizer_spacy.vocab, \"LOWER\")\n",
    "for cui, group_df in tqdm(df_subset.groupby(\"cui\")):\n",
    "    snomed_matcher.add(cui, list(tokenizer_spacy.pipe(group_df[\"name\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = tokenizer_spacy(\"heart attack\")\n",
    "matches = snomed_matcher(doc)\n",
    "tokenizer_spacy.vocab.strings[matches[0][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_cuis = 0\n",
    "dataset_annotations = []\n",
    "for doc in tqdm(dataset[:1000]):\n",
    "    doc_annotations = []\n",
    "\n",
    "    bhc_cui_ids = {\n",
    "        match_id\n",
    "        for match_id, _, _ in snomed_matcher(tokenizer_spacy(doc.discharge_summary.bhc))\n",
    "    }\n",
    "\n",
    "    for note in doc.physician_notes:\n",
    "        for section in note.text.split(\"\\n\\n\"):\n",
    "            for sentence in re.split(\"\\n(?=[^ a-z])|(?<=[?|!|.])\\\\s\", section):\n",
    "                sentence_cui_ids = {\n",
    "                    match_id\n",
    "                    for match_id, _, _ in snomed_matcher(tokenizer_spacy(sentence))\n",
    "                }\n",
    "                if not sentence_cui_ids:\n",
    "                    no_cuis += 1\n",
    "                    continue\n",
    "                elif sentence_cui_ids.intersection(bhc_cui_ids) == set():\n",
    "                    label = \"NEGATIVE\"\n",
    "                else:\n",
    "                    label = \"POSITIVE\"\n",
    "                doc_annotations.append(\n",
    "                    {\"text\": sentence, \"labels\": label, \"cui_ids\": sentence_cui_ids}\n",
    "                )\n",
    "        dataset_annotations.append(doc_annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_dataset_labels = [\n",
    "    str(sentence_annotation[\"labels\"])\n",
    "    for doc_annotations in dataset_annotations\n",
    "    for sentence_annotation in doc_annotations\n",
    "]\n",
    "len(flattened_dataset_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sentences = len(flattened_dataset_labels) + no_cuis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label, count in Counter(flattened_dataset_labels).most_common():\n",
    "    print(label, count / num_sentences)\n",
    "print(\"NONE\", no_cuis / num_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bhc_cuis = 0\n",
    "num_matches = 0\n",
    "\n",
    "for doc, docs_annotations in tqdm(zip(dataset, dataset_annotations)):\n",
    "    note_cui_ids = {\n",
    "        cui_id\n",
    "        for sentence_annotation in docs_annotations\n",
    "        for cui_id in sentence_annotation[\"cui_ids\"]\n",
    "    }\n",
    "\n",
    "    bhc_cui_ids = {\n",
    "        match_id\n",
    "        for match_id, _, _ in snomed_matcher(tokenizer_spacy(doc.discharge_summary.bhc))\n",
    "    }\n",
    "    num_matches += len(note_cui_ids.intersection(bhc_cui_ids))\n",
    "    num_bhc_cuis += len(bhc_cui_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_matches / num_bhc_cuis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
