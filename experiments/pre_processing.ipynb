{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from discharge_summaries.schemas.mimic import DischargeSummary, Note, Record\n",
    "from discharge_summaries.schemas.output import Paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 23\n",
    "DATA_DIR = Path.cwd().parent / \"data\"\n",
    "MIMIC_DIR = DATA_DIR / \"physionet.org\" / \"files\"\n",
    "\n",
    "MIMIC_III_DIR = MIMIC_DIR / \"mimiciii\" / \"1.4\"\n",
    "MIMIC_IV_DIR = MIMIC_DIR / \"mimiciv\" / \"2.2\" / \"note\"\n",
    "\n",
    "TRAIN_SAVE_PATH = DATA_DIR / \"train.pkl\"\n",
    "TEST_SAVE_PATH = DATA_DIR / \"test.pkl\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in MIMIC III notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = pd.read_csv(MIMIC_III_DIR / \"NOTEEVENTS.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre processing\n",
    "\n",
    "Remove error and duplicate rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = full_df[full_df[\"ISERROR\"] != 1]\n",
    "full_df.drop(\"ISERROR\", axis=1, inplace=True)\n",
    "full_df = full_df.drop_duplicates()\n",
    "full_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(full_df), len(full_df[\"HADM_ID\"].unique())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep only Physician and discharge notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df[\"CATEGORY\"].unique()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group by HADM_ID and keep ones with DS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = full_df.groupby(\"HADM_ID\")\n",
    "df = grouped_df.filter(\n",
    "    lambda group: all(\n",
    "        item in group[\"CATEGORY\"].values for item in [\"Discharge summary\", \"Physician \"]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    # Tidy up new lines\n",
    "    cleaned_text = re.sub(r\"\\n\\.\\n\", r\"\\n\\n\", text)\n",
    "    cleaned_text = re.sub(r\"\\n {2,}\", \"\\n\", cleaned_text)\n",
    "    cleaned_text = re.sub(r\"\\n{3,}\", \"\\n\\n\", cleaned_text)\n",
    "    # Make text paragraphs be on 1 line\n",
    "    cleaned_text = re.sub(r\"\\n *(?=[a-z])\", \" \", cleaned_text)\n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "df[\"TEXT\"] = df[\"TEXT\"].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"CHARTTIME\"] = df[\"CHARTTIME\"].fillna(full_df[\"CHARTDATE\"] + \" 23:59:59\")\n",
    "df = df.sort_values(by=[\"HADM_ID\", \"CHARTTIME\"])\n",
    "df = df.reset_index(drop=True)\n",
    "len(df), len(df[\"HADM_ID\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_bhc(discharge_summary_text: str) -> str:\n",
    "    start_pattern = r\"\\nBrief Hospital Course:\\n\"\n",
    "    end_pattern = r\"\\nMedications on Admission:\\n\"\n",
    "    # Match any characters between the start and end pattern\n",
    "    match = re.search(\n",
    "        f\"{start_pattern}(.*?){end_pattern}\", discharge_summary_text, re.DOTALL\n",
    "    )\n",
    "    if not match:\n",
    "        return \"\"\n",
    "    return match.group(1)\n",
    "\n",
    "\n",
    "def extract_bhc_paragraphs(bhc: str) -> List[Paragraph]:\n",
    "    bhc_paragraphs = []\n",
    "    # # or a digit, followed by non-alpha characters (split on)\n",
    "    # followed by an alpha character and then anything then a ; or - or : before new line starts\n",
    "    heading_punctuation = r\"[;:\\-\\.]\"\n",
    "    heading_regex_pattern = (\n",
    "        f\"(?:#|\\\\d[\\\\.|\\\\)])[^a-zA-Z]*?(?=[a-zA-Z][^\\n]*?(?:{heading_punctuation})[\"\n",
    "        \" |\\n])\"\n",
    "    )\n",
    "    for idx, paragraph_text in enumerate(\n",
    "        re.split(\"\\n\\n\" + heading_regex_pattern, bhc.strip())\n",
    "    ):\n",
    "        if \"\\n\\n\" in paragraph_text:\n",
    "            return []\n",
    "        if idx == 0:\n",
    "            attempt_first_para_split = re.split(\n",
    "                \"^\" + heading_regex_pattern, paragraph_text, maxsplit=1\n",
    "            )\n",
    "            paragraph_text = (\n",
    "                attempt_first_para_split[1]\n",
    "                if len(attempt_first_para_split) > 1\n",
    "                else \": \" + paragraph_text\n",
    "            )\n",
    "        split = re.split(f\"{heading_punctuation}[ |\\n]\", paragraph_text, maxsplit=1)\n",
    "        bhc_paragraphs.append(\n",
    "            Paragraph(\n",
    "                heading=split[0].strip(),\n",
    "                text=split[1].strip() if len(split) > 1 else \"\",\n",
    "            )\n",
    "        )\n",
    "    return bhc_paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "missing_bhc, missing_paragraphs = 0, 0\n",
    "for hadm_id, group_df in tqdm(df.groupby(\"HADM_ID\")):\n",
    "    physician_notes = [\n",
    "        Note(\n",
    "            text=series[\"TEXT\"],\n",
    "            datetime=series[\"CHARTTIME\"],\n",
    "            category=series[\"CATEGORY\"],\n",
    "            description=series[\"DESCRIPTION\"],\n",
    "        )\n",
    "        for _, series in group_df[group_df[\"CATEGORY\"] == \"Physician \"].iterrows()\n",
    "    ]\n",
    "\n",
    "    discharge_summary_row = group_df[group_df[\"CATEGORY\"] == \"Discharge summary\"].iloc[\n",
    "        0\n",
    "    ]\n",
    "    bhc = extract_bhc(discharge_summary_row[\"TEXT\"])\n",
    "    bhc_paragraphs = extract_bhc_paragraphs(bhc)\n",
    "    if not bhc:\n",
    "        missing_bhc += 1\n",
    "        continue\n",
    "    if len(bhc_paragraphs) <= 1:\n",
    "        missing_paragraphs += 1\n",
    "        continue\n",
    "\n",
    "    discharge_summary = DischargeSummary(\n",
    "        text=discharge_summary_row[\"TEXT\"],\n",
    "        datetime=discharge_summary_row[\"CHARTTIME\"],\n",
    "        category=discharge_summary_row[\"CATEGORY\"],\n",
    "        description=discharge_summary_row[\"DESCRIPTION\"],\n",
    "        bhc=bhc,\n",
    "        bhc_paragraphs=bhc_paragraphs,\n",
    "    )\n",
    "\n",
    "    record = Record(\n",
    "        physician_notes=sorted(physician_notes),\n",
    "        discharge_summary=discharge_summary,\n",
    "        hadm_id=hadm_id,\n",
    "        subject_id=group_df[\"SUBJECT_ID\"].iloc[0],\n",
    "    )\n",
    "    dataset.append(record)\n",
    "len(dataset), missing_bhc, missing_paragraphs, len(df.groupby(\"HADM_ID\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = train_test_split(\n",
    "    dataset, test_size=0.2, random_state=RANDOM_SEED\n",
    ")\n",
    "len(train_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(TRAIN_SAVE_PATH, \"wb\") as out_file:\n",
    "    pickle.dump([record.dict() for record in train_dataset], out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(TEST_SAVE_PATH, \"wb\") as out_file:\n",
    "    pickle.dump([record.dict() for record in test_dataset], out_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
