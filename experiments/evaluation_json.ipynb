{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Callable, Dict, List\n",
    "\n",
    "import numpy as np\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "# from rouge_score import rouge_scorer\n",
    "# from summac.model_summac import SummaCZS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = Path().cwd() / \"output\"\n",
    "output_gt_path = output_dir / \"prsb_example_gt.json\"\n",
    "output_pred_path = output_dir / \"prsb_example.json\"\n",
    "json_schema_path = (\n",
    "    Path().cwd().parent\n",
    "    / \"guidelines\"\n",
    "    / \"eDischarge-Summary-v2.1-1st-Feb-21_schema.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_gt_json = json.loads(output_gt_path.read_text())[\"Completion\"]\n",
    "output_pred_json = json.loads(output_pred_path.read_text())[\"Completion\"]\n",
    "json_schema = json.loads(json_schema_path.read_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_metric(pred_value: str, gt_value: str) -> float:\n",
    "    return 1 if pred_value == gt_value else 0\n",
    "\n",
    "\n",
    "def evaluate_array(\n",
    "    pred_array: List[Dict],\n",
    "    gt_array: List[Dict],\n",
    "    array_schema: Dict,\n",
    "    calc_metric: Callable[[str, str], float],\n",
    ") -> Dict[str, float]:\n",
    "    array_fields = list(array_schema[\"items\"][\"properties\"].keys())\n",
    "    if len(gt_array) == 0 and len(pred_array) == 0:\n",
    "        return {field: 1 for field in array_fields}\n",
    "    elif len(gt_array) == 0 or len(pred_array) == 0:\n",
    "        return {field: 0 for field in array_fields}\n",
    "    else:\n",
    "        array_metrics: Dict[str, List[float]] = {field: [] for field in array_fields}\n",
    "\n",
    "    main_key = array_fields[0]\n",
    "    cost = np.array(\n",
    "        [\n",
    "            [calc_metric(pred[main_key], gt[main_key]) for pred in pred_array]\n",
    "            for gt in gt_array\n",
    "        ]\n",
    "    )\n",
    "    pred_idxs, gt_idxs = linear_sum_assignment(cost, maximize=True)\n",
    "\n",
    "    for pred_idx, gt_idx in zip(pred_idxs, gt_idxs):\n",
    "        for element_name, metric_value in evaluate_json(\n",
    "            pred_array[pred_idx], gt_array[gt_idx], array_schema[\"items\"], calc_metric\n",
    "        ).items():\n",
    "            array_metrics[element_name].append(metric_value)\n",
    "    return {\n",
    "        field: np.mean(metric_values) if len(metric_values) > 1 else metric_values[0]\n",
    "        for field, metric_values in array_metrics.items()\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluate_json(\n",
    "    pred_json: Dict,\n",
    "    gt_json: Dict,\n",
    "    json_schema: Dict,\n",
    "    calc_metric: Callable[[str, str], float],\n",
    "):\n",
    "    metrics = {}\n",
    "    # print(pred_discharge_summary, gt_discharge_summary, json_schema)\n",
    "    for section_name, section_schema in json_schema[\"properties\"].items():\n",
    "        if section_schema[\"type\"] == \"object\":\n",
    "            metrics[section_name] = evaluate_json(\n",
    "                pred_json[section_name],\n",
    "                gt_json[section_name],\n",
    "                section_schema,\n",
    "                calc_metric,\n",
    "            )\n",
    "        elif section_schema[\"type\"] == \"array\":\n",
    "            metrics[section_name] = evaluate_array(\n",
    "                pred_json[section_name],\n",
    "                gt_json[section_name],\n",
    "                section_schema,\n",
    "                calc_metric,\n",
    "            )\n",
    "        elif section_schema[\"type\"] == \"string\":\n",
    "            metrics[section_name] = calc_metric(\n",
    "                pred_json[section_name], gt_json[section_name]\n",
    "            )\n",
    "    return metrics\n",
    "\n",
    "\n",
    "evaluate_json(output_pred_json, output_gt_json, json_schema, dummy_metric)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
