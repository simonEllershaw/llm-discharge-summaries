{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from typing import Callable, Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# from rouge_score import rouge_scorer\n",
    "# from summac.model_summac import SummaCZS\n",
    "from bert_score import BERTScorer\n",
    "from medcat.cat import CAT\n",
    "from scipy.optimize import linear_sum_assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = Path().cwd() / \"output\"\n",
    "output_gt_path = output_dir / \"prsb_example_gt_manual.json\"\n",
    "output_pred_path = output_dir / \"prsb_example_completion.json\"\n",
    "json_schema_path = (\n",
    "    Path().cwd().parent\n",
    "    / \"guidelines\"\n",
    "    / \"eDischarge-Summary-v2.1-1st-Feb-21_pydantic.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_gt_json = json.loads(output_gt_path.read_text())  # [\"Completion\"]\n",
    "output_pred_json = json.loads(output_pred_path.read_text())  # [\"Completion\"]\n",
    "json_schema = json.loads(json_schema_path.read_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_jsons = [output_pred_json for _ in range(10)]\n",
    "gt_jsons = [output_gt_json for _ in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_object_metrics_to_section_dict(\n",
    "    object_metrics: Dict[str, float], metrics: Dict[str, float], section_name: str\n",
    "):\n",
    "    for metric_name, metric_value in object_metrics.items():\n",
    "        metrics[f\"{section_name}__{metric_name}\"] = metric_value\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def evaluate_string_array(\n",
    "    pred_array: List[str], gt_array: List[str], calc_metric: Callable[[str, str], float]\n",
    ") -> float:\n",
    "    if len(gt_array) == 0 and len(pred_array) == 0:\n",
    "        return 1\n",
    "    elif len(gt_array) == 0 or len(pred_array) == 0:\n",
    "        return 0\n",
    "\n",
    "    cost = np.array([[calc_metric(pred, gt) for pred in pred_array] for gt in gt_array])\n",
    "    pred_idxs, gt_idxs = linear_sum_assignment(cost, maximize=True)\n",
    "    return cost[pred_idxs, gt_idxs].mean()\n",
    "\n",
    "\n",
    "def evaluate_object_array(\n",
    "    pred_array: List[Dict],\n",
    "    gt_array: List[Dict],\n",
    "    object_schema: Dict,\n",
    "    calc_metric: Callable[[str, str], float],\n",
    ") -> Dict[str, float]:\n",
    "    object_properties = list(object_schema[\"properties\"].keys())\n",
    "    if len(gt_array) == 0 and len(pred_array) == 0:\n",
    "        return {field: 1 for field in object_properties}\n",
    "    elif len(gt_array) == 0 or len(pred_array) == 0:\n",
    "        return {field: 0 for field in object_properties}\n",
    "\n",
    "    array_metrics: Dict[str, List[float]] = defaultdict(list)\n",
    "    main_key = object_properties[0]\n",
    "    cost = np.array(\n",
    "        [\n",
    "            [calc_metric(pred[main_key], gt[main_key]) for pred in pred_array]\n",
    "            for gt in gt_array\n",
    "        ]\n",
    "    )\n",
    "    pred_idxs, gt_idxs = linear_sum_assignment(cost, maximize=True)\n",
    "\n",
    "    for pred_idx, gt_idx in zip(pred_idxs, gt_idxs):\n",
    "        for element_name, metric_value in evaluate_json(\n",
    "            pred_array[pred_idx], gt_array[gt_idx], object_schema, calc_metric\n",
    "        ).items():\n",
    "            if type(metric_value) == dict:\n",
    "                for field, value in metric_value.items():\n",
    "                    array_metrics[f\"{element_name}__{field}\"].append(value)\n",
    "            else:\n",
    "                array_metrics[element_name].append(metric_value)\n",
    "    return {\n",
    "        field: np.mean(metric_values) if len(metric_values) > 1 else metric_values[0]\n",
    "        for field, metric_values in array_metrics.items()\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluate_json(\n",
    "    pred_json: Dict,\n",
    "    gt_json: Dict,\n",
    "    json_schema: Dict,\n",
    "    calc_metric: Callable[[str, str], float],\n",
    ") -> Dict[str, float]:\n",
    "    metrics: Dict[str, float] = {}\n",
    "    # print(pred_discharge_summary, gt_discharge_summary, json_schema)\n",
    "    for section_name, section_schema in json_schema[\"properties\"].items():\n",
    "        if section_schema[\"type\"] == \"object\":\n",
    "            object_metrics = evaluate_json(\n",
    "                pred_json[section_name],\n",
    "                gt_json[section_name],\n",
    "                section_schema,\n",
    "                calc_metric,\n",
    "            )\n",
    "            metrics = add_object_metrics_to_section_dict(\n",
    "                object_metrics, metrics, section_name\n",
    "            )\n",
    "        elif section_schema[\"type\"] == \"array\":\n",
    "            if section_schema[\"items\"][\"type\"] == \"object\":\n",
    "                object_metrics = evaluate_object_array(\n",
    "                    pred_json[section_name],\n",
    "                    gt_json[section_name],\n",
    "                    section_schema[\"items\"],\n",
    "                    calc_metric,\n",
    "                )\n",
    "                metrics = add_object_metrics_to_section_dict(\n",
    "                    object_metrics, metrics, section_name\n",
    "                )\n",
    "            elif section_schema[\"items\"][\"type\"] == \"string\":\n",
    "                metrics[section_name] = evaluate_string_array(\n",
    "                    pred_json[section_name],\n",
    "                    gt_json[section_name],\n",
    "                    calc_metric,\n",
    "                )\n",
    "        elif section_schema[\"type\"] == \"string\":\n",
    "            metrics[section_name] = calc_metric(\n",
    "                pred_json[section_name], gt_json[section_name]\n",
    "            )\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def average_per_field_metrics(field_metrics: Dict[str, float]) -> float:\n",
    "    return sum(field_metrics.values()) / len(field_metrics)\n",
    "\n",
    "\n",
    "def run_per_field_dataset_metrics(\n",
    "    pred_jsons: List[Dict],\n",
    "    gt_jsons: List[Dict],\n",
    "    metric_function: Callable[[str, str], float],\n",
    ") -> Tuple[Dict[str, float], float]:\n",
    "    metric_list_per_field = defaultdict(list)\n",
    "    for pred_json, output_json in zip(pred_jsons, gt_jsons):\n",
    "        metrics = evaluate_json(pred_json, output_json, json_schema, metric_function)\n",
    "        for field, value in metrics.items():\n",
    "            metric_list_per_field[field].append(value)\n",
    "    metrics_per_field = {\n",
    "        field: np.array(values).mean()\n",
    "        for field, values in metric_list_per_field.items()\n",
    "    }\n",
    "    return metrics_per_field, average_per_field_metrics(metrics_per_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_content_metric(pred_value: str, gt_value: str) -> float:\n",
    "    if pred_value and not gt_value:\n",
    "        return 0\n",
    "    elif not pred_value and gt_value:\n",
    "        return 0\n",
    "    return 1\n",
    "\n",
    "\n",
    "has_content_per_field_metrics, average_has_content = run_per_field_dataset_metrics(\n",
    "    pred_jsons, gt_jsons, has_content_metric\n",
    ")\n",
    "print(average_has_content)\n",
    "has_content_per_field_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_bert_score(pred_value: str, gt_value: str, bert_scorer) -> float:\n",
    "    if not pred_value and not gt_value:\n",
    "        return 1\n",
    "    elif not pred_value or not gt_value:\n",
    "        return 0\n",
    "    _, _, F1 = bert_scorer.score([pred_value], [gt_value])\n",
    "    return F1.mean()\n",
    "\n",
    "\n",
    "scorer = BERTScorer(lang=\"en\", rescale_with_baseline=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_score_per_field_metrics, average_bert_score = run_per_field_dataset_metrics(\n",
    "    pred_jsons, gt_jsons, lambda pred, gt: calc_bert_score(pred, gt, scorer)\n",
    ")\n",
    "print(average_bert_score)\n",
    "bert_score_per_field_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = (\n",
    "    Path.cwd().parent\n",
    "    / \"models\"\n",
    "    / \"mc_modelpack_snomed_int_16_mar_2022_25be3857ba34bdd5.zip\"\n",
    ")\n",
    "\n",
    "cat = CAT.load_model_pack(MODEL_PATH)\n",
    "cat.pipe.force_remove(\"Status\")\n",
    "cat.pipe.spacy_nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_snomed_match(pred_value: str, gt_value: str, cat: CAT) -> float:\n",
    "    pred_cuis = {ent._.cui for ent in cat(pred_value).ents} if pred_value else set()\n",
    "    gt_cuis = {ent._.cui for ent in cat(gt_value).ents} if gt_value else set()\n",
    "\n",
    "    precision = (\n",
    "        len(pred_cuis.intersection(gt_cuis)) / len(pred_cuis) if pred_cuis else 1\n",
    "    )\n",
    "    recall = len(gt_cuis.intersection(pred_cuis)) / len(gt_cuis) if gt_cuis else 1\n",
    "    if precision + recall == 0:\n",
    "        return 0\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snomed_per_field_metrics, average_snomed = run_per_field_dataset_metrics(\n",
    "    pred_jsons, gt_jsons, lambda pred, gt: calc_snomed_match(pred, gt, cat)\n",
    ")\n",
    "print(average_snomed)\n",
    "snomed_per_field_metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
