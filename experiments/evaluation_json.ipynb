{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from typing import Callable, Dict, List\n",
    "\n",
    "import numpy as np\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "# from rouge_score import rouge_scorer\n",
    "# from summac.model_summac import SummaCZS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = Path().cwd() / \"output\"\n",
    "output_gt_path = output_dir / \"prsb_example_gt_manual.json\"\n",
    "output_pred_path = output_dir / \"prsb_example_completion.json\"\n",
    "json_schema_path = (\n",
    "    Path().cwd().parent\n",
    "    / \"guidelines\"\n",
    "    / \"eDischarge-Summary-v2.1-1st-Feb-21_pydantic.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_gt_json = json.loads(output_gt_path.read_text())  # [\"Completion\"]\n",
    "output_pred_json = json.loads(output_pred_path.read_text())  # [\"Completion\"]\n",
    "json_schema = json.loads(json_schema_path.read_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_content_metric(pred_value: str, gt_value: str) -> float:\n",
    "    if pred_value and not gt_value:\n",
    "        return 0\n",
    "    elif not pred_value and gt_value:\n",
    "        return 0\n",
    "    return 1\n",
    "\n",
    "\n",
    "def add_object_metrics_to_section_dict(\n",
    "    object_metrics: Dict[str, float], metrics: Dict[str, float], section_name: str\n",
    "):\n",
    "    for metric_name, metric_value in object_metrics.items():\n",
    "        metrics[f\"{section_name}__{metric_name}\"] = metric_value\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def evaluate_string_array(\n",
    "    pred_array: List[str], gt_array: List[str], calc_metric: Callable[[str, str], float]\n",
    ") -> float:\n",
    "    if len(gt_array) == 0 and len(pred_array) == 0:\n",
    "        return 1\n",
    "    elif len(gt_array) == 0 or len(pred_array) == 0:\n",
    "        return 0\n",
    "\n",
    "    cost = np.array([[calc_metric(pred, gt) for pred in pred_array] for gt in gt_array])\n",
    "    pred_idxs, gt_idxs = linear_sum_assignment(cost, maximize=True)\n",
    "    return cost[pred_idxs, gt_idxs].mean()\n",
    "\n",
    "\n",
    "def evaluate_object_array(\n",
    "    pred_array: List[Dict],\n",
    "    gt_array: List[Dict],\n",
    "    object_schema: Dict,\n",
    "    calc_metric: Callable[[str, str], float],\n",
    ") -> Dict[str, float]:\n",
    "    object_properties = list(object_schema[\"properties\"].keys())\n",
    "    if len(gt_array) == 0 and len(pred_array) == 0:\n",
    "        return {field: 1 for field in object_properties}\n",
    "    elif len(gt_array) == 0 or len(pred_array) == 0:\n",
    "        return {field: 0 for field in object_properties}\n",
    "\n",
    "    array_metrics: Dict[str, List[float]] = defaultdict(list)\n",
    "    main_key = object_properties[0]\n",
    "    cost = np.array(\n",
    "        [\n",
    "            [calc_metric(pred[main_key], gt[main_key]) for pred in pred_array]\n",
    "            for gt in gt_array\n",
    "        ]\n",
    "    )\n",
    "    pred_idxs, gt_idxs = linear_sum_assignment(cost, maximize=True)\n",
    "\n",
    "    for pred_idx, gt_idx in zip(pred_idxs, gt_idxs):\n",
    "        for element_name, metric_value in evaluate_json(\n",
    "            pred_array[pred_idx], gt_array[gt_idx], object_schema, calc_metric\n",
    "        ).items():\n",
    "            if type(metric_value) == dict:\n",
    "                for field, value in metric_value.items():\n",
    "                    array_metrics[f\"{element_name}__{field}\"].append(value)\n",
    "            else:\n",
    "                array_metrics[element_name].append(metric_value)\n",
    "    return {\n",
    "        field: np.mean(metric_values) if len(metric_values) > 1 else metric_values[0]\n",
    "        for field, metric_values in array_metrics.items()\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluate_json(\n",
    "    pred_json: Dict,\n",
    "    gt_json: Dict,\n",
    "    json_schema: Dict,\n",
    "    calc_metric: Callable[[str, str], float],\n",
    "):\n",
    "    metrics: Dict[str, float] = {}\n",
    "    # print(pred_discharge_summary, gt_discharge_summary, json_schema)\n",
    "    for section_name, section_schema in json_schema[\"properties\"].items():\n",
    "        if section_schema[\"type\"] == \"object\":\n",
    "            object_metrics = evaluate_json(\n",
    "                pred_json[section_name],\n",
    "                gt_json[section_name],\n",
    "                section_schema,\n",
    "                calc_metric,\n",
    "            )\n",
    "            metrics = add_object_metrics_to_section_dict(\n",
    "                object_metrics, metrics, section_name\n",
    "            )\n",
    "        elif section_schema[\"type\"] == \"array\":\n",
    "            if section_schema[\"items\"][\"type\"] == \"object\":\n",
    "                object_metrics = evaluate_object_array(\n",
    "                    pred_json[section_name],\n",
    "                    gt_json[section_name],\n",
    "                    section_schema[\"items\"],\n",
    "                    calc_metric,\n",
    "                )\n",
    "                metrics = add_object_metrics_to_section_dict(\n",
    "                    object_metrics, metrics, section_name\n",
    "                )\n",
    "            elif section_schema[\"items\"][\"type\"] == \"string\":\n",
    "                metrics[section_name] = evaluate_string_array(\n",
    "                    pred_json[section_name],\n",
    "                    gt_json[section_name],\n",
    "                    calc_metric,\n",
    "                )\n",
    "        elif section_schema[\"type\"] == \"string\":\n",
    "            metrics[section_name] = calc_metric(\n",
    "                pred_json[section_name], gt_json[section_name]\n",
    "            )\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_per_field = evaluate_json(\n",
    "    output_pred_json, output_gt_json, json_schema, has_content_metric\n",
    ")\n",
    "print(np.mean(list(metrics_per_field.values())))\n",
    "metrics_per_field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
