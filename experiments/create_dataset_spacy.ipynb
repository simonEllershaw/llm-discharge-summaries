{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import re\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "\n",
    "from spacy.lang.en import English\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.training import biluo_to_iob, offsets_to_biluo_tags\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from discharge_summaries.preprocessing.preprocess_snomed import Snomed\n",
    "from discharge_summaries.schemas.mimic import Record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path.cwd().parent / \"data\"\n",
    "\n",
    "TRAINING_DATASET_PATH = DATA_DIR / \"train.pkl\"\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y_%m_%d_%H_%M\")\n",
    "TRAINING_ANNO_DATASET_PATH = DATA_DIR / f\"train_anno_{TIMESTAMP}.pkl\"\n",
    "DATASET_NOTE_CUI_CACHE_PATH = DATA_DIR / \"dataset_note_cui_cache.json\"\n",
    "MODEL_PATH = (\n",
    "    Path.cwd().parent\n",
    "    / \"models\"\n",
    "    / \"mc_modelpack_snomed_int_16_mar_2022_25be3857ba34bdd5.zip\"\n",
    ")\n",
    "RANDOM_SEED = 23\n",
    "LOG_FILE = \"./medcat.log\"\n",
    "DIRECT_LABEL = \"DIRECT\"\n",
    "SNOMED_PATH = (\n",
    "    Path.cwd().parent / \"data\" / \"SnomedCT_InternationalRF2_PRODUCTION_20230731T120000Z\"\n",
    ")\n",
    "\n",
    "SPACY_MODEL = \"en_core_sci_md\"\n",
    "MAX_SEGMENT_TOKEN_LENGTH = 400\n",
    "HF_MODEL_NAME = \"roberta-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(TRAINING_DATASET_PATH, \"rb\") as in_file:\n",
    "    dataset = [Record(**record) for record in pickle.load(in_file)]\n",
    "dataset = dataset\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing SNOMED CT for MedCAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sowmed = Snomed(str(SNOMED_PATH))\n",
    "sowmed.uk_ext = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sowmed.to_concept_df()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"description_type_ids\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_type_names = {\n",
    "    \"disorder\",\n",
    "    \"finding\",\n",
    "    \"morphologic abnormality\",\n",
    "    \"organism\",\n",
    "    \"physical object\",\n",
    "    \"clinical drug\",\n",
    "    \"medicinal product form\",\n",
    "    \"procedure\",\n",
    "    \"product\",\n",
    "}\n",
    "assert all(name in df[\"description_type_ids\"].unique() for name in filter_type_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[\"description_type_ids\"].isin(filter_type_names)]\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_subset = df[df['description_type_ids'].isin(['finding', 'disorder'])]\n",
    "df_subset = df[df[\"name_status\"] == \"A\"]\n",
    "len(df_subset), len(df_subset[\"cui\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_spacy = English().tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snomed_matcher = PhraseMatcher(tokenizer_spacy.vocab, \"LOWER\")\n",
    "for cui, group_df in tqdm(df_subset.groupby(\"cui\")):\n",
    "    snomed_matcher.add(cui, list(tokenizer_spacy.pipe(group_df[\"name\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = tokenizer_spacy(\"heart attack\")\n",
    "matches = snomed_matcher(doc)\n",
    "tokenizer_spacy.vocab.strings[matches[0][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_note_into_chunks(\n",
    "    note_text: str, max_segment_token_length: int, tokenizer: AutoTokenizer\n",
    ") -> List[str]:\n",
    "    chunks = []\n",
    "    for section in note_text.split(\"\\n\\n\"):\n",
    "        chunk_token_length = 0\n",
    "        chunk = \"\"\n",
    "        for line in re.split(\"\\n(?=[^ a-z])|(?<=\\\\.)\\\\s\", section):\n",
    "            line_token_length = len(tokenizer(line)[\"input_ids\"])\n",
    "            if line_token_length > max_segment_token_length:\n",
    "                raise ValueError(line)\n",
    "            if chunk_token_length + line_token_length < max_segment_token_length:\n",
    "                chunk += f\"\\n{line}\"\n",
    "                chunk_token_length += line_token_length\n",
    "            else:\n",
    "                chunks.append(chunk.strip())\n",
    "                chunk = line\n",
    "                chunk_token_length = line_token_length\n",
    "        # Final chunk\n",
    "        chunks.append(chunk.strip())\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve_overlapping_matches(\n",
    "    matches: List[Tuple[str, int, int]]\n",
    ") -> List[Tuple[str, int, int]]:\n",
    "    matches.sort(key=lambda match: match[2] - match[1], reverse=True)\n",
    "\n",
    "    merged_matches: List[Tuple[str, int, int]] = []\n",
    "    for match in matches:\n",
    "        overlap = False\n",
    "        for existing_match in merged_matches:\n",
    "            if match[1] < existing_match[2] and match[2] > existing_match[1]:\n",
    "                overlap = True\n",
    "                if len(match) >= len(existing_match):\n",
    "                    merged_matches.remove(existing_match)\n",
    "                    merged_matches.append(match)\n",
    "                break\n",
    "\n",
    "        if not overlap:\n",
    "            merged_matches.append(match)\n",
    "\n",
    "    return sorted(merged_matches, key=lambda match: match[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_hf = AutoTokenizer.from_pretrained(\n",
    "    HF_MODEL_NAME, add_prefix_space=True, use_fast=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_annotations = []\n",
    "for doc in tqdm(dataset):\n",
    "    doc_annotations = []\n",
    "\n",
    "    doc_headings = [\n",
    "        para.heading for para in doc.discharge_summary.bhc_paragraphs if para.heading\n",
    "    ]\n",
    "    para_cuis = {\n",
    "        match.label_\n",
    "        for match in snomed_matcher(\n",
    "            tokenizer_spacy(\"\\n\\n\".join(doc_headings)), as_spans=True\n",
    "        )\n",
    "    }\n",
    "\n",
    "    doc_matcher = PhraseMatcher(tokenizer_spacy.vocab, attr=\"LOWER\")\n",
    "    for heading in doc_headings:\n",
    "        doc_matcher.add(\n",
    "            f\"{DIRECT_LABEL}-{heading}\", list(tokenizer_spacy.pipe([heading]))\n",
    "        )\n",
    "    for cui in para_cuis:\n",
    "        doc_matcher.add(\n",
    "            cui, list(tokenizer_spacy.pipe(df_subset[df_subset[\"cui\"] == cui][\"name\"]))\n",
    "        )\n",
    "\n",
    "    for note in doc.physician_notes:\n",
    "        for chunk in split_note_into_chunks(\n",
    "            note.text, MAX_SEGMENT_TOKEN_LENGTH, tokenizer_hf\n",
    "        ):\n",
    "            spacy_chunk = tokenizer_spacy(chunk)\n",
    "            matches = doc_matcher(spacy_chunk)\n",
    "            resolved_matches = resolve_overlapping_matches(matches)\n",
    "            offsets = [\n",
    "                (\n",
    "                    spacy_chunk[start_token:end_token].start_char,\n",
    "                    spacy_chunk[start_token:end_token].end_char,\n",
    "                    tokenizer_spacy.vocab.strings[match_id],\n",
    "                )\n",
    "                for match_id, start_token, end_token in resolved_matches\n",
    "            ]\n",
    "            iob_annotations = biluo_to_iob(offsets_to_biluo_tags(spacy_chunk, offsets))\n",
    "            tokens = [token.text for token in spacy_chunk]\n",
    "            doc_annotations.append({\"tokens\": tokens, \"ner_tags\": iob_annotations})\n",
    "    dataset_annotations.append(doc_annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_annotations = {\n",
    "    token\n",
    "    for chunk_annos in dataset_annotations[0]\n",
    "    for token, tag in zip(chunk_annos[\"tokens\"], chunk_annos[\"ner_tags\"])\n",
    "    if tag != \"O\"\n",
    "}\n",
    "headings = sorted(\n",
    "    [\n",
    "        para.heading\n",
    "        for para in dataset[0].discharge_summary.bhc_paragraphs\n",
    "        if para.heading\n",
    "    ]\n",
    ")\n",
    "text_annotations, headings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_headings = 0\n",
    "num_matches = 0\n",
    "cui_hits = []\n",
    "partial_cui_hit = []\n",
    "strict_match = []\n",
    "no_match = []\n",
    "\n",
    "for doc, docs_annotations in tqdm(zip(dataset, dataset_annotations)):\n",
    "    doc_anno_cuis = {\n",
    "        tag[len(\"B-\") :]\n",
    "        for chunk_annotations in docs_annotations\n",
    "        for tag in chunk_annotations[\"ner_tags\"]\n",
    "        if tag != \"O\" and DIRECT_LABEL not in tag\n",
    "    }\n",
    "    doc_anno_direct_text = {\n",
    "        tag[len(f\"B-{DIRECT_LABEL}-\") :].lower()\n",
    "        for chunk_annotations in docs_annotations\n",
    "        for tag in chunk_annotations[\"ner_tags\"]\n",
    "        if DIRECT_LABEL in tag\n",
    "    }\n",
    "\n",
    "    for para in doc.discharge_summary.bhc_paragraphs:\n",
    "        if not para.heading:\n",
    "            continue\n",
    "        num_headings += 1\n",
    "        para_cuis = {\n",
    "            tokenizer_spacy.vocab.strings[label_id]\n",
    "            for label_id, _, _ in snomed_matcher(tokenizer_spacy(para.heading))\n",
    "        }\n",
    "        if para_cuis and para_cuis.issubset(doc_anno_cuis):\n",
    "            cui_hits.append(para.heading)\n",
    "        elif para_cuis.intersection(doc_anno_cuis) != set():\n",
    "            partial_cui_hit.append(para.heading)\n",
    "        elif para.heading.lower() in doc_anno_direct_text:\n",
    "            strict_match.append(para.heading)\n",
    "        else:\n",
    "            no_match.append(para.heading)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_hit_rate = (\n",
    "    len(cui_hits) + len(partial_cui_hit) + len(strict_match)\n",
    ") / num_headings\n",
    "cui_hit_rate = len(cui_hits) / num_headings\n",
    "partial_cui_hit_rate = len(partial_cui_hit) / num_headings\n",
    "strict_match_rate = len(strict_match) / num_headings\n",
    "no_match_rate = len(no_match) / num_headings\n",
    "\n",
    "total_hit_rate, cui_hit_rate, partial_cui_hit_rate, strict_match_rate, no_match_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(no_match).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(TRAINING_ANNO_DATASET_PATH, \"wb\") as out_file:\n",
    "    pickle.dump(dataset_annotations, out_file)\n",
    "str(TRAINING_ANNO_DATASET_PATH.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_annos = sum(\n",
    "    1\n",
    "    for doc_annos in dataset_annotations\n",
    "    for chunk_annos in doc_annos\n",
    "    for tag in chunk_annos[\"ner_tags\"]\n",
    "    if tag != \"O\"\n",
    ")\n",
    "num_tags = sum(\n",
    "    len(chunk_annos[\"ner_tags\"])\n",
    "    for doc_annos in dataset_annotations\n",
    "    for chunk_annos in doc_annos\n",
    ")\n",
    "num_annos / num_tags * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_annos_chunks = sum(\n",
    "    1\n",
    "    for doc_annos in dataset_annotations\n",
    "    for chunk_annos in doc_annos\n",
    "    if set(chunk_annos[\"ner_tags\"]) != {\"O\"}\n",
    ")\n",
    "num_chunks = sum(1 for doc_annos in dataset_annotations for chunk in doc_annos)\n",
    "num_annos_chunks / num_chunks * 100"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
