{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "from medcat.utils.preprocess_snomed import Snomed\n",
    "from spacy.lang.en import English\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Span\n",
    "from spacy.training import biluo_to_iob, offsets_to_biluo_tags\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from discharge_summaries.schemas.mimic import Record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path.cwd().parent / \"data\"\n",
    "\n",
    "TRAINING_DATASET_PATH = DATA_DIR / \"train.pkl\"\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y_%m_%d_%H_%M\")\n",
    "TRAINING_ANNO_DATASET_PATH = DATA_DIR / f\"train_anno_{TIMESTAMP}.pkl\"\n",
    "DATASET_NOTE_CUI_CACHE_PATH = DATA_DIR / \"dataset_note_cui_cache.json\"\n",
    "MODEL_PATH = (\n",
    "    Path.cwd().parent\n",
    "    / \"models\"\n",
    "    / \"mc_modelpack_snomed_int_16_mar_2022_25be3857ba34bdd5.zip\"\n",
    ")\n",
    "RANDOM_SEED = 23\n",
    "LOG_FILE = \"./medcat.log\"\n",
    "DIRECT_LABEL = \"DIRECT\"\n",
    "SNOMED_PATH = (\n",
    "    Path.cwd().parent / \"data\" / \"SnomedCT_InternationalRF2_PRODUCTION_20230731T120000Z\"\n",
    ")\n",
    "\n",
    "SPACY_MODEL = \"en_core_web_md\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(TRAINING_DATASET_PATH, \"rb\") as in_file:\n",
    "    dataset = [Record(**record) for record in pickle.load(in_file)]\n",
    "dataset = dataset\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing SNOMED CT for MedCAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sowmed = Snomed(str(SNOMED_PATH))\n",
    "sowmed.uk_ext = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sowmed.to_concept_df()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"description_type_ids\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_type_names = {\n",
    "    \"disorder\",\n",
    "    \"finding\",\n",
    "    \"morphologic abnormality\",\n",
    "    \"organism\",\n",
    "    \"physical object\",\n",
    "    \"clinical drug\",\n",
    "    \"medicinal product form\",\n",
    "    \"procedure\",\n",
    "    \"product\",\n",
    "}\n",
    "assert all(name in df[\"description_type_ids\"].unique() for name in filter_type_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[\"description_type_ids\"].isin(filter_type_names)]\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_subset = df[df['description_type_ids'].isin(['finding', 'disorder'])]\n",
    "df_subset = df[df[\"name_status\"] == \"A\"]\n",
    "len(df_subset), len(df_subset[\"cui\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = English().tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snomed_matcher = PhraseMatcher(tokenizer.vocab, \"LOWER\")\n",
    "for cui, group_df in tqdm(df_subset.groupby(\"cui\")):\n",
    "    snomed_matcher.add(cui, list(tokenizer.pipe(group_df[\"name\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = snomed_matcher(tokenizer(\"heart attack\"), as_spans=True)\n",
    "matches[0].label_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve_overlapping_spans(spans: List[Span]) -> List[Span]:\n",
    "    spans.sort(key=lambda span: len(span), reverse=True)\n",
    "\n",
    "    merged_spans: List[Span] = []\n",
    "    for span in spans:\n",
    "        overlap = False\n",
    "        for existing_span in merged_spans:\n",
    "            if span.start < existing_span.end and span.end > existing_span.start:\n",
    "                overlap = True\n",
    "                if len(span) >= len(existing_span):\n",
    "                    merged_spans.remove(existing_span)\n",
    "                    merged_spans.append(span)\n",
    "                break\n",
    "\n",
    "        if not overlap:\n",
    "            merged_spans.append(span)\n",
    "\n",
    "    return sorted(merged_spans, key=lambda span: span.start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_annotations = []\n",
    "for doc in tqdm(dataset):\n",
    "    doc_annotations = []\n",
    "\n",
    "    doc_headings = [\n",
    "        para.heading for para in doc.discharge_summary.bhc_paragraphs if para.heading\n",
    "    ]\n",
    "    para_cuis = {\n",
    "        match.label_\n",
    "        for match in snomed_matcher(tokenizer(\"\\n\\n\".join(doc_headings)), as_spans=True)\n",
    "    }\n",
    "\n",
    "    doc_matcher = PhraseMatcher(tokenizer.vocab, attr=\"LOWER\")\n",
    "    doc_matcher.add(\"DIRECT\", list(tokenizer.pipe(doc_headings)))\n",
    "    for cui in para_cuis:\n",
    "        doc_matcher.add(\n",
    "            cui, list(tokenizer.pipe(df_subset[df_subset[\"cui\"] == cui][\"name\"]))\n",
    "        )\n",
    "\n",
    "    for note in doc.physician_notes:\n",
    "        spacy_note = tokenizer(note.text)\n",
    "        spans = doc_matcher(spacy_note, as_spans=True)\n",
    "        resolved_spans = resolve_overlapping_spans(spans)\n",
    "        doc_annotations.append({\"doc\": spacy_note, \"spans\": resolved_spans})\n",
    "    dataset_annotations.append(doc_annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_annotations = {\n",
    "    anno.text for note_annos in dataset_annotations[0] for anno in note_annos[\"spans\"]\n",
    "}\n",
    "headings = sorted(\n",
    "    [\n",
    "        para.heading\n",
    "        for para in dataset[0].discharge_summary.bhc_paragraphs\n",
    "        if para.heading\n",
    "    ]\n",
    ")\n",
    "text_annotations, headings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_headings = 0\n",
    "num_matches = 0\n",
    "cui_hits = []\n",
    "partial_cui_hit = []\n",
    "strict_match = []\n",
    "no_match = []\n",
    "\n",
    "for doc, docs_annotations in tqdm(zip(dataset, dataset_annotations)):\n",
    "    doc_anno_cuis = {\n",
    "        anno.label_\n",
    "        for note_annotations in docs_annotations\n",
    "        for anno in note_annotations[\"spans\"]\n",
    "        if anno.label_ != DIRECT_LABEL\n",
    "    }\n",
    "    doc_anno_direct_text = {\n",
    "        anno.text.lower()\n",
    "        for note_annotations in docs_annotations\n",
    "        for anno in note_annotations[\"spans\"]\n",
    "        if anno.label_ == DIRECT_LABEL\n",
    "    }\n",
    "    for para in doc.discharge_summary.bhc_paragraphs:\n",
    "        if not para.heading:\n",
    "            continue\n",
    "        num_headings += 1\n",
    "        para_cuis = {\n",
    "            match.label_\n",
    "            for match in snomed_matcher(tokenizer(para.heading), as_spans=True)\n",
    "        }\n",
    "        if para_cuis and para_cuis.issubset(doc_anno_cuis):\n",
    "            cui_hits.append(para.heading)\n",
    "        elif para_cuis.intersection(doc_anno_cuis) != set():\n",
    "            partial_cui_hit.append(para.heading)\n",
    "        elif para.heading.lower() in doc_anno_direct_text:\n",
    "            strict_match.append(para.heading)\n",
    "        else:\n",
    "            no_match.append(para.heading)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_hit_rate = (\n",
    "    len(cui_hits) + len(partial_cui_hit) + len(strict_match)\n",
    ") / num_headings\n",
    "cui_hit_rate = len(cui_hits) / num_headings\n",
    "partial_cui_hit_rate = len(partial_cui_hit) / num_headings\n",
    "strict_match_rate = len(strict_match) / num_headings\n",
    "no_match_rate = len(no_match) / num_headings\n",
    "\n",
    "total_hit_rate, cui_hit_rate, partial_cui_hit_rate, strict_match_rate, no_match_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(no_match).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_iob_annotations = []\n",
    "for doc_annotations in dataset_annotations:\n",
    "    doc_iob_annotations = []\n",
    "    for note_annotations in doc_annotations:\n",
    "        offsets = [\n",
    "            (anno.start_char, anno.end_char, \"PRIORITY\")\n",
    "            for anno in note_annotations[\"spans\"]\n",
    "        ]\n",
    "        iob_annotations = biluo_to_iob(\n",
    "            offsets_to_biluo_tags(note_annotations[\"doc\"], offsets)\n",
    "        )\n",
    "        tokens = [token.text for token in note_annotations[\"doc\"]]\n",
    "        doc_iob_annotations.append({\"tokens\": tokens, \"ner_tags\": iob_annotations})\n",
    "    dataset_iob_annotations.append(doc_iob_annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for para_iob_annotations in dataset_iob_annotations[0][:100]:\n",
    "    for token, tag in zip(\n",
    "        para_iob_annotations[\"tokens\"], para_iob_annotations[\"ner_tags\"]\n",
    "    ):\n",
    "        if tag != \"O\":\n",
    "            print(repr(token), tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(TRAINING_ANNO_DATASET_PATH, \"wb\") as out_file:\n",
    "    pickle.dump(dataset_iob_annotations, out_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
