{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from discharge_summaries.schemas.mimic import DischargeSummary, Note, Record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 23\n",
    "DATA_DIR = Path.cwd().parent / \"data\"\n",
    "MIMIC_DIR = DATA_DIR / \"physionet.org\" / \"files\"\n",
    "\n",
    "MIMIC_III_DIR = MIMIC_DIR / \"mimiciii\" / \"1.4\"\n",
    "MIMIC_IV_DIR = MIMIC_DIR / \"mimiciv\" / \"2.2\" / \"note\"\n",
    "\n",
    "TRAIN_SAVE_PATH = DATA_DIR / \"train_all_ds.pkl\"\n",
    "TEST_SAVE_PATH = DATA_DIR / \"test_all_ds.pkl\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in MIMIC III notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = pd.read_csv(MIMIC_III_DIR / \"NOTEEVENTS.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre processing\n",
    "\n",
    "Remove error and duplicate rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = full_df[full_df[\"ISERROR\"] != 1]\n",
    "full_df.drop(\"ISERROR\", axis=1, inplace=True)\n",
    "full_df = full_df.drop_duplicates()\n",
    "full_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(full_df), len(full_df[\"HADM_ID\"].unique())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep only Physician and discharge notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df[\"CATEGORY\"].unique()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group by HADM_ID and keep ones with DS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = full_df.groupby(\"HADM_ID\")\n",
    "df = grouped_df.filter(\n",
    "    lambda group: all(\n",
    "        item in group[\"CATEGORY\"].values for item in [\"Discharge summary\", \"Physician \"]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    # Tidy up new lines\n",
    "    cleaned_text = re.sub(r\"\\n {2,}\", \"\\n\", text)\n",
    "    cleaned_text = re.sub(r\"\\n{3,}\", \"\\n\\n\", cleaned_text)\n",
    "    cleaned_text = re.sub(r\"\\n\\.\\n\", r\"\\n\\n\", cleaned_text)\n",
    "    # Remove de-id tags\n",
    "    cleaned_text = re.sub(r\"(?:\\[\\*\\*)|(?:\\*\\*\\])\", \"\", cleaned_text)\n",
    "    cleaned_text = re.sub(r\" {2,}\", \" \", cleaned_text)\n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "df[\"TEXT\"] = df[\"TEXT\"].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"CHARTTIME\"] = df[\"CHARTTIME\"].fillna(full_df[\"CHARTDATE\"] + \" 23:59:59\")\n",
    "df = df.sort_values(by=[\"HADM_ID\", \"CHARTTIME\"])\n",
    "df = df.reset_index(drop=True)\n",
    "len(df), len(df[\"HADM_ID\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_bhc(discharge_summary_text: str) -> str:\n",
    "    start_pattern = r\"\\nBrief Hospital Course:\\n\"\n",
    "    end_pattern = r\"\\nMedications on Admission:\\n\"\n",
    "    # Match any characters between the start and end pattern\n",
    "    match = re.search(\n",
    "        f\"{start_pattern}(.*?){end_pattern}\", discharge_summary_text, re.DOTALL\n",
    "    )\n",
    "    if not match:\n",
    "        return \"\"\n",
    "    return match.group(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "missing_bhc = 0\n",
    "no_report = 0\n",
    "for hadm_id, group_df in tqdm(df.groupby(\"HADM_ID\")):\n",
    "    physician_notes = [\n",
    "        Note(\n",
    "            text=series[\"TEXT\"],\n",
    "            datetime=series[\"CHARTTIME\"],\n",
    "            category=series[\"CATEGORY\"],\n",
    "            description=series[\"DESCRIPTION\"],\n",
    "        )\n",
    "        for _, series in group_df[group_df[\"CATEGORY\"] == \"Physician \"].iterrows()\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        discharge_summary_row = group_df[\n",
    "            (group_df[\"CATEGORY\"] == \"Discharge summary\")\n",
    "            & (group_df[\"DESCRIPTION\"] == \"Report\")\n",
    "        ].iloc[0]\n",
    "    except IndexError:\n",
    "        no_report += 1\n",
    "    bhc = extract_bhc(discharge_summary_row[\"TEXT\"])\n",
    "    if not bhc:\n",
    "        missing_bhc += 1\n",
    "        continue\n",
    "\n",
    "    discharge_summary = DischargeSummary(\n",
    "        text=discharge_summary_row[\"TEXT\"],\n",
    "        datetime=discharge_summary_row[\"CHARTTIME\"],\n",
    "        category=discharge_summary_row[\"CATEGORY\"],\n",
    "        description=discharge_summary_row[\"DESCRIPTION\"],\n",
    "        bhc=bhc,\n",
    "        bhc_paragraphs=[],\n",
    "    )\n",
    "\n",
    "    record = Record(\n",
    "        physician_notes=sorted(physician_notes),\n",
    "        discharge_summary=discharge_summary,\n",
    "        hadm_id=hadm_id,\n",
    "        subject_id=group_df[\"SUBJECT_ID\"].iloc[0],\n",
    "    )\n",
    "    dataset.append(record)\n",
    "len(dataset), missing_bhc, no_report, len(df.groupby(\"HADM_ID\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = train_test_split(\n",
    "    dataset, test_size=0.2, random_state=RANDOM_SEED\n",
    ")\n",
    "len(train_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(TRAIN_SAVE_PATH, \"wb\") as out_file:\n",
    "    pickle.dump([record.dict() for record in train_dataset], out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(TEST_SAVE_PATH, \"wb\") as out_file:\n",
    "    pickle.dump([record.dict() for record in test_dataset], out_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
