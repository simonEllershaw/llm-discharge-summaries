{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bert_score import BERTScorer\n",
    "from dotenv import load_dotenv\n",
    "from medcat.cat import CAT\n",
    "from rouge_score import rouge_scorer\n",
    "from summac.model_summac import SummaCZS\n",
    "\n",
    "from discharge_summaries.schemas.medcat import MedCATSpan\n",
    "from discharge_summaries.schemas.mimic import Record\n",
    "from discharge_summaries.schemas.output import Paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "UMLS_API_KEY = os.environ.get(\"UMLS_API_KEY\")\n",
    "\n",
    "UMLS_BASE_URL = \"https://uts-ws.nlm.nih.gov/rest\"\n",
    "\n",
    "DATA_DIR = Path.cwd().parent / \"data\"\n",
    "GT_DATA_PATH = DATA_DIR / \"train.pkl\"\n",
    "PRED_DATA_PATH = Path.cwd() / \"output\" / \"2023_07_18_17_57.json\"\n",
    "\n",
    "MODEL_PATH = Path.cwd().parent / \"models\" / \"umls_sm_pt2ch_533bab5115c6c2d6.zip\"\n",
    "\n",
    "OPEN_API_VERSION = \"2023-05-15\"\n",
    "DEPLOYMENT_NAME = \"gpt-35-turbo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(GT_DATA_PATH, \"rb\") as in_file:\n",
    "    gt_dataset = [Record(**record) for record in pickle.load(in_file)]\n",
    "gt_dataset = gt_dataset[:10]\n",
    "len(gt_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_bhcs_full = [sample.discharge_summary.bhc for sample in gt_dataset]\n",
    "gt_bhcs_paras = [sample.discharge_summary.bhc_paragraphs for sample in gt_dataset]\n",
    "len(gt_bhcs_full), len(gt_bhcs_paras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gt_bhcs_full[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with PRED_DATA_PATH.open() as file_in:\n",
    "    pred_bhcs_paras = [\n",
    "        [Paragraph(**para) for para in bhc] for bhc in json.load(file_in)\n",
    "    ]\n",
    "pred_bhcs_full = [\n",
    "    \"\\n\\n\".join(f\"# {para.heading}: {para.text}\" for para in bhc)\n",
    "    for bhc in pred_bhcs_paras\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(gt_bhcs_paras), len(pred_bhcs_paras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_sample = gt_bhcs_paras[0]\n",
    "gt_headings = [para.heading for para in gt_sample[1:]]\n",
    "sorted(gt_headings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = CAT.load_model_pack(MODEL_PATH)\n",
    "# type_ids_filter = [\"T047\"]\n",
    "# cui_filters = {\n",
    "#     cui\n",
    "#     for type_ids in type_ids_filter\n",
    "#     for cui in cat.cdb.addl_info[\"type_id2cuis\"][type_ids]\n",
    "# }\n",
    "# cat.cdb.config.linking[\"filters\"][\"cuis\"] = cui_filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_cuis(text: str, cat: CAT) -> set[str]:\n",
    "    annotated_text = cat(text)\n",
    "    return (\n",
    "        {\n",
    "            MedCATSpan.from_spacy_span(ent, cat, context=\"\").cui\n",
    "            for ent in annotated_text.ents\n",
    "        }\n",
    "        if annotated_text\n",
    "        else set()\n",
    "    )\n",
    "\n",
    "\n",
    "def extract_cuis_from_bhc_headings(bhc: list[Paragraph], cat: CAT) -> set[str]:\n",
    "    return {cui for bhc_para in bhc for cui in extract_cuis(bhc_para.heading, cat)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cui_to_name(cui: str, cat: CAT) -> str:\n",
    "    return cat.cdb.get_name(cui)\n",
    "\n",
    "\n",
    "def cuis_to_names(cuis: set[str], cat: CAT) -> list[str]:\n",
    "    return sorted(cat.cdb.get_name(cui) for cui in cuis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CUI Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_heading_cuis_to_paras = [\n",
    "    {cui: para for para in paras[1:] for cui in extract_cuis(para.heading, cat)}\n",
    "    for paras in gt_bhcs_paras\n",
    "]\n",
    "pred_heading_cuis_to_paras = [\n",
    "    {cui: para for para in paras for cui in extract_cuis(para.heading, cat)}\n",
    "    for paras in pred_bhcs_paras\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gt_bhcs_full[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat.get_entities(\"hand\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, (gt_heading_cuis_to_para, pred_heading_cuis_to_para) in enumerate(\n",
    "    zip(gt_heading_cuis_to_paras, pred_heading_cuis_to_paras)\n",
    "):\n",
    "    missed = set(gt_heading_cuis_to_para.keys()) - set(pred_heading_cuis_to_para.keys())\n",
    "    if missed:\n",
    "        print(idx)\n",
    "        print(sorted(cuis_to_names(missed, cat)))\n",
    "        print(sorted(cuis_to_names(set(pred_heading_cuis_to_para.keys()), cat)))\n",
    "    # hit = set(gt_heading_cuis_to_para.keys()).intersection(set(pred_heading_cuis_to_para.keys()))\n",
    "    # print(sorted(hit))\n",
    "    # print(sorted(gt_heading_cuis_to_para.keys()))\n",
    "    # print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for gt_sample, pred_sample in zip(gt_bhcs_paras[:1], pred_bhcs_paras[:1]):\n",
    "    pred_heading_to_evidence = {\n",
    "        para.heading.lower(): para.evidence for para in pred_sample\n",
    "    }\n",
    "    for gt_para in gt_sample:\n",
    "        if gt_para.heading.lower()[2:] not in pred_heading_to_evidence.keys():\n",
    "            print(gt_para.heading)\n",
    "            # print(\"---\")\n",
    "            # print(gt_para.text)\n",
    "            # print(\"---\")\n",
    "            # for evidence in pred_heading_to_evidence[gt_para.heading.lower()[2:]]:\n",
    "            #     print(evidence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prefix agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_cuis = [\n",
    "    extract_cuis_from_bhc_headings(gt_bhc_paras, cat) for gt_bhc_paras in gt_bhcs_paras\n",
    "]\n",
    "pred_cuis = [\n",
    "    extract_cuis_from_bhc_headings(pred_bhc_paras, cat)\n",
    "    for pred_bhc_paras in pred_bhcs_paras\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hits = 0\n",
    "num_gts = 0\n",
    "\n",
    "for gt, pred in zip(gt_cuis, pred_cuis):\n",
    "    num_hits += len(gt.intersection(pred))\n",
    "    num_gts += len(gt)\n",
    "\n",
    "    print(cuis_to_names(gt - gt.intersection(pred), cat))\n",
    "\n",
    "num_hits / num_gts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in gt_dataset[-1].physician_notes:\n",
    "    print(text.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Level Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_text_length(texts: list[str]) -> float:\n",
    "    return np.array([len(text.split()) for text in texts]).mean()\n",
    "\n",
    "\n",
    "average_text_length(gt_bhcs_full), average_text_length(pred_bhcs_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_rouge_score(gold: list[str], pred: list[str]) -> pd.DataFrame:\n",
    "    scorer = rouge_scorer.RougeScorer(\n",
    "        [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"], use_stemmer=True\n",
    "    )\n",
    "    scores: dict[str, dict[str, list[float]]] = defaultdict(\n",
    "        lambda: {\"precision\": [], \"recall\": [], \"f1\": []}\n",
    "    )\n",
    "    for sample_gold, sample_pred in zip(gold, pred):\n",
    "        for metric, score in scorer.score(sample_gold, sample_pred).items():\n",
    "            scores[metric][\"precision\"].append(score.precision)\n",
    "            scores[metric][\"recall\"].append(score.recall)\n",
    "            scores[metric][\"f1\"].append(score.fmeasure)\n",
    "\n",
    "    for metric, score in scores.items():\n",
    "        for name, value in score.items():\n",
    "            scores[metric][name] = np.array(value).mean()\n",
    "\n",
    "    return pd.DataFrame(scores).T\n",
    "\n",
    "\n",
    "calc_rouge_score(gt_bhcs_full, pred_bhcs_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_scorer_full = BERTScorer(\n",
    "    model_type=\"microsoft/deberta-xlarge-mnli\",\n",
    "    lang=\"en\",\n",
    "    rescale_with_baseline=True,\n",
    "    idf=True,\n",
    "    idf_sents=gt_bhcs_full,\n",
    ")\n",
    "P, R, F1 = bert_scorer_full.score(gt_bhcs_full, pred_bhcs_full, verbose=True)\n",
    "P.mean(), R.mean(), F1.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paragraph level metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_paragraphs = []\n",
    "pred_paragraphs = []\n",
    "evidence = []\n",
    "\n",
    "for gt_bhc_paras, pred_bhc_paras in zip(gt_bhcs_paras, pred_bhcs_paras):\n",
    "    pred_cui_to_para = {\n",
    "        next(iter(extract_cuis(pred_para.heading, cat))): pred_para\n",
    "        for pred_para in pred_bhc_paras\n",
    "    }\n",
    "    for gt_para in gt_bhc_paras[1:]:  # TODO add 1st para\n",
    "        gt_para_cuis = extract_cuis(gt_para.heading, cat)\n",
    "        matched_paras = [\n",
    "            pred_cui_to_para[cui].text\n",
    "            for cui in gt_para_cuis\n",
    "            if cui in pred_cui_to_para\n",
    "        ]\n",
    "        extracts = [\n",
    "            extract\n",
    "            for cui in gt_para_cuis\n",
    "            if cui in pred_cui_to_para\n",
    "            for extract in pred_cui_to_para[cui].evidence\n",
    "        ]\n",
    "        if matched_paras:\n",
    "            gt_paragraphs.append(gt_para.text)\n",
    "            pred_paragraphs.append(\"\\n\".join(matched_paras))\n",
    "            evidence.append(\"\\n\\n\".join(extracts))\n",
    "\n",
    "len([para for gt_bhc_paras in gt_bhcs_paras for para in gt_bhc_paras]), len(\n",
    "    gt_paragraphs\n",
    "), len(pred_paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_rouge_score(gt_paragraphs, pred_paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_text_length(gt_paragraphs), average_text_length(pred_paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_zs = SummaCZS(\n",
    "    granularity=\"paragraph\", model_name=\"vitc\", device=\"cpu\"\n",
    ")  # If you have a GPU: switch to: device=\"cuda\"\n",
    "# TODO: add SummaCConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_zs1 = model_zs.score(evidence[:1], pred_paragraphs[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_zs1[\"scores\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(evidence[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with np.printoptions(precision=3, suppress=True):\n",
    "    print(score_zs1[\"images\"][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
