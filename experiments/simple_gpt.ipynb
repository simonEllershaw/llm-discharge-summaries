{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import tiktoken\n",
    "from dotenv import load_dotenv\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from discharge_summaries.openai_llm.chat_models import AzureOpenAIChatModel\n",
    "from discharge_summaries.openai_llm.message import Message, Role\n",
    "from discharge_summaries.schemas.mimic import Note, Record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path.cwd().parent / \"data\"\n",
    "OUTPUT_DIR = Path.cwd() / \"output\"\n",
    "\n",
    "\n",
    "TRAINING_DATASET_PATH = DATA_DIR / \"train_all_ds.pkl\"\n",
    "RANDOM_SEED = 23\n",
    "AZURE_ENGINE = \"gpt-4-32k\"\n",
    "AZURE_API_VERSION = \"2023-07-01-preview\"\n",
    "# AZURE_ENGINE = \"gpt-35-turbo\"\n",
    "# AZURE_API_VERSION = \"2023-07-01-preview\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_tokens_from_messages_azure_engine(\n",
    "    messages: List[Message], azure_engine: str, azure_api_version: str\n",
    ") -> int:\n",
    "    azure_engine_and_version_to_openai_model = {\n",
    "        \"gpt-35-turbo-2023-07-01-preview\": \"gpt-3.5-turbo-0613\",\n",
    "        \"gpt-4-32k-2023-07-01-preview\": \"gpt-4-32k-0613\",\n",
    "    }\n",
    "    try:\n",
    "        model = azure_engine_and_version_to_openai_model[\n",
    "            f\"{azure_engine}-{azure_api_version}\"\n",
    "        ]\n",
    "    except KeyError:\n",
    "        raise NotImplementedError(\n",
    "            \"num_tokens_from_messages() is not implemented for model\"\n",
    "            f\" {azure_engine}-{azure_api_version}.\"\n",
    "        )\n",
    "    return num_tokens_from_messages(messages, model=model)\n",
    "\n",
    "\n",
    "def num_tokens_from_messages(messages: List[Message], model: str) -> int:\n",
    "    \"\"\"Return the number of tokens used by a list of messages.\"\"\"\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "    except KeyError:\n",
    "        print(\"Warning: model not found. Using cl100k_base encoding.\")\n",
    "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    if model in {\n",
    "        \"gpt-3.5-turbo-0613\",\n",
    "        \"gpt-3.5-turbo-16k-0613\",\n",
    "        \"gpt-4-0314\",\n",
    "        \"gpt-4-32k-0314\",\n",
    "        \"gpt-4-0613\",\n",
    "        \"gpt-4-32k-0613\",\n",
    "    }:\n",
    "        tokens_per_message = 3\n",
    "        tokens_per_name = 1\n",
    "    elif model == \"gpt-3.5-turbo-0301\":\n",
    "        tokens_per_message = (\n",
    "            4  # every message follows <|start|>{role/name}\\n{content}<|end|>\\n\n",
    "        )\n",
    "        tokens_per_name = -1  # if there's a name, the role is omitted\n",
    "    elif \"gpt-3.5-turbo\" in model:\n",
    "        print(\n",
    "            \"Warning: gpt-3.5-turbo may update over time. Returning num tokens assuming\"\n",
    "            \" gpt-3.5-turbo-0613.\"\n",
    "        )\n",
    "        return num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0613\")\n",
    "    elif \"gpt-4\" in model:\n",
    "        print(\n",
    "            \"Warning: gpt-4 may update over time. Returning num tokens assuming\"\n",
    "            \" gpt-4-0613.\"\n",
    "        )\n",
    "        return num_tokens_from_messages(messages, model=\"gpt-4-0613\")\n",
    "    else:\n",
    "        raise NotImplementedError(\n",
    "            f\"\"\"num_tokens_from_messages() is not implemented for model {model}.\n",
    "            See https://github.com/openai/openai-python/blob/main/chatml.md for information\n",
    "            on how messages are converted to tokens.\"\"\"\n",
    "        )\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += tokens_per_message\n",
    "        for key, value in message.dict().items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":\n",
    "                num_tokens += tokens_per_name\n",
    "    num_tokens += 3  # every reply is primed with <|start|>assistant<|message|>\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(TRAINING_DATASET_PATH, \"rb\") as in_file:\n",
    "    dataset = [Record(**record) for record in pickle.load(in_file)]\n",
    "dataset = dataset\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bhc_token_lengths = np.array(\n",
    "    [\n",
    "        num_tokens_from_messages_azure_engine(\n",
    "            [Message(role=Role.ASSISTANT, content=record.discharge_summary.bhc)],\n",
    "            AZURE_ENGINE,\n",
    "            AZURE_API_VERSION,\n",
    "        )\n",
    "        for record in dataset\n",
    "    ]\n",
    ")\n",
    "np.mean(bhc_token_lengths), np.median(bhc_token_lengths), np.std(\n",
    "    bhc_token_lengths\n",
    "), np.min(bhc_token_lengths), np.max(bhc_token_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bhc_token_lengths = np.array(\n",
    "    [\n",
    "        num_tokens_from_messages_azure_engine(\n",
    "            [Message(role=Role.ASSISTANT, content=record.discharge_summary.bhc)],\n",
    "            AZURE_ENGINE,\n",
    "            AZURE_API_VERSION,\n",
    "        )\n",
    "        for record in tqdm(dataset)\n",
    "    ]\n",
    ")\n",
    "np.median(bhc_token_lengths), np.min(bhc_token_lengths), np.max(\n",
    "    bhc_token_lengths\n",
    "), np.percentile(bhc_token_lengths, 95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "note_token_lengths = [\n",
    "    [\n",
    "        num_tokens_from_messages_azure_engine(\n",
    "            [Message(role=Role.USER, content=note.text)],\n",
    "            AZURE_ENGINE,\n",
    "            AZURE_API_VERSION,\n",
    "        )\n",
    "        for note in record.physician_notes\n",
    "    ]\n",
    "    for record in tqdm(dataset)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_note_token_lengths = np.array(\n",
    "    [\n",
    "        note_token_length\n",
    "        for record_note_lengths in note_token_lengths\n",
    "        for note_token_length in record_note_lengths\n",
    "    ]\n",
    ")\n",
    "np.median(single_note_token_lengths), np.min(single_note_token_lengths), np.max(\n",
    "    single_note_token_lengths\n",
    "), np.percentile(single_note_token_lengths, 95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_note_token_lengths = np.array(\n",
    "    [sum(record_note_lengths) for record_note_lengths in note_token_lengths]\n",
    ")\n",
    "np.median(combined_note_token_lengths), np.min(combined_note_token_lengths), np.max(\n",
    "    combined_note_token_lengths\n",
    "), np.percentile(combined_note_token_lengths, 95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_notes = np.array([len(record.physician_notes) for record in tqdm(dataset)])\n",
    "np.median(num_notes), np.min(num_notes), np.max(num_notes), np.percentile(num_notes, 95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_filtered_95 = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(1 for length in combined_note_token_lengths if length < 31000) / len(\n",
    "    combined_note_token_lengths\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rough_cost_per_generation = 31 * 0.047 + 1 * 0.094\n",
    "rough_cost_per_generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Generation 32K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = AzureOpenAIChatModel(\n",
    "    api_base=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_KEY\"),\n",
    "    api_version=AZURE_API_VERSION,\n",
    "    engine=AZURE_ENGINE,\n",
    "    temperature=0,\n",
    "    timeout=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_MESSAGE = Message(\n",
    "    role=Role.SYSTEM,\n",
    "    content=f\"\"\"You are a consultant doctor.\n",
    "You are tasked with writing the brief hospital course summary section of a patients discharge summary.\n",
    "To aid you in this task the user provides you with physician notes from the patient's visit.\n",
    "Each note has a title of the format Physician Note [number]: [timestamp].\n",
    "The summary should be roughly 500 words long.\n",
    "You can only use the information in the notes to write the summary.\n",
    "\n",
    "This is an example of a brief hospital course summary:\n",
    "{dataset[0].discharge_summary.bhc}\n",
    "\"\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_notes_string(notes: List[Note]):\n",
    "    return \"\\n\\n\".join(\n",
    "        f\"Physician Note {idx+1}: {note.datetime}\\n{note.text}\"\n",
    "        for idx, note in enumerate(notes)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = []\n",
    "for idx, record in enumerate(dataset[1:]):\n",
    "    note_string = generate_notes_string(record.physician_notes)\n",
    "    user_message_content = \"\"\"Write a brief hopsital course summary from the following physician notes.\n",
    "\n",
    "{note_string}\"\"\"\n",
    "    user_message = Message(role=Role.USER, content=user_message_content)\n",
    "    num_user_tokens = num_tokens_from_messages_azure_engine(\n",
    "        [user_message], AZURE_ENGINE, AZURE_API_VERSION\n",
    "    )\n",
    "    if num_user_tokens > 31000:\n",
    "        print(f\"Skipping record {idx+1} because it has {num_user_tokens} tokens.\")\n",
    "        continue\n",
    "    response = llm.query([SYSTEM_MESSAGE, user_message])\n",
    "    outputs.append((idx + 1, [SYSTEM_MESSAGE, user_message, response]))\n",
    "    if len(outputs) > 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, messages in outputs:\n",
    "    prompts = \"\\n\\n*****\\n\\n\".join(\n",
    "        [f\"{message.role}:\\n{message.content}\" for message in messages[:2]]\n",
    "    )\n",
    "    file_output = (\n",
    "        f\"GPT BHC:\\n{messages[-1].content}\\n\\n*****\\n\\nHuman\"\n",
    "        f\" BHC:\\n{dataset[idx].discharge_summary.bhc}\\n\\n*****\\n\\nPrompt:{prompts}\"\n",
    "    )\n",
    "    (OUTPUT_DIR / f\"{idx}.txt\").write_text(file_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
