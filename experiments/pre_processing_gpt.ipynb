{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from discharge_summaries.openai_llm.chat_models import AzureOpenAIChatModel\n",
    "from discharge_summaries.openai_llm.prompts import (\n",
    "    generate_bhc_paragraph_heading_extraction_prompt,\n",
    ")\n",
    "from discharge_summaries.schemas.mimic import DischargeSummary, Note, Record\n",
    "from discharge_summaries.schemas.output import Paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 23\n",
    "DATA_DIR = Path.cwd().parent / \"data\"\n",
    "MIMIC_DIR = DATA_DIR / \"physionet.org\" / \"files\"\n",
    "\n",
    "MIMIC_III_DIR = MIMIC_DIR / \"mimiciii\" / \"1.4\"\n",
    "MIMIC_IV_DIR = MIMIC_DIR / \"mimiciv\" / \"2.2\" / \"note\"\n",
    "\n",
    "TRAIN_SAVE_PATH = DATA_DIR / \"train.pkl\"\n",
    "TEST_SAVE_PATH = DATA_DIR / \"test.pkl\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in MIMIC III notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = pd.read_csv(MIMIC_III_DIR / \"NOTEEVENTS.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre processing\n",
    "\n",
    "Remove error and duplicate rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = full_df[full_df[\"ISERROR\"] != 1]\n",
    "full_df.drop(\"ISERROR\", axis=1, inplace=True)\n",
    "full_df = full_df.drop_duplicates()\n",
    "full_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(full_df), len(full_df[\"HADM_ID\"].unique())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep only Physician and discharge notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df[\"CATEGORY\"].unique()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group by HADM_ID and keep ones with DS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = full_df.groupby(\"HADM_ID\")\n",
    "df = grouped_df.filter(\n",
    "    lambda group: all(\n",
    "        item in group[\"CATEGORY\"].values for item in [\"Discharge summary\", \"Physician \"]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    # Tidy up new lines\n",
    "    cleaned_text = re.sub(r\"\\n\\.\\n\", r\"\\n\\n\", text)\n",
    "    cleaned_text = re.sub(r\"\\n {2,}\", \"\\n\", cleaned_text)\n",
    "    cleaned_text = re.sub(r\"\\n{3,}\", \"\\n\\n\", cleaned_text)\n",
    "    # Make text paragraphs be on 1 line\n",
    "    cleaned_text = re.sub(r\"\\n *(?=[a-z])\", \" \", cleaned_text)\n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "df[\"TEXT\"] = df[\"TEXT\"].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"CHARTTIME\"] = df[\"CHARTTIME\"].fillna(full_df[\"CHARTDATE\"] + \" 23:59:59\")\n",
    "df = df.sort_values(by=[\"HADM_ID\", \"CHARTTIME\"])\n",
    "df = df.reset_index(drop=True)\n",
    "len(df), len(df[\"HADM_ID\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_bhc(discharge_summary_text: str) -> str:\n",
    "    start_pattern = r\"\\nBrief Hospital Course:\\n\"\n",
    "    end_pattern = r\"\\nMedications on Admission:\\n\"\n",
    "    # Match any characters between the start and end pattern\n",
    "    match = re.search(\n",
    "        f\"{start_pattern}(.*?){end_pattern}\", discharge_summary_text, re.DOTALL\n",
    "    )\n",
    "    if not match:\n",
    "        return \"\"\n",
    "    return match.group(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = AzureOpenAIChatModel(\n",
    "    api_base=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_KEY\"),\n",
    "    api_version=\"2023-05-15\",\n",
    "    engine=\"gpt-35-turbo\",\n",
    "    temperature=0.7,\n",
    "    timeout=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, first_group_df = next(iter(df.groupby(\"HADM_ID\")))\n",
    "discharge_summary_row = first_group_df[\n",
    "    first_group_df[\"CATEGORY\"] == \"Discharge summary\"\n",
    "].iloc[0]\n",
    "bhc = extract_bhc(discharge_summary_row[\"TEXT\"])\n",
    "bhc_paragraphs = bhc.split(\"\\n\\n\")\n",
    "few_shot_examples = [\n",
    "    (bhc_paragraphs[0], \"None\"),\n",
    "    (bhc_paragraphs[1], \"UGIB\"),\n",
    "    (\"Code: Full\", \"Code\"),\n",
    "]\n",
    "few_shot_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def extract_bhc_paragraphs(bhc: str) -> List[Paragraph]:\n",
    "    prompts = [\n",
    "        generate_bhc_paragraph_heading_extraction_prompt(bhc_para, few_shot_examples)\n",
    "        for bhc_para in bhc.split(\"\\n\\n\")\n",
    "    ]\n",
    "    tasks = [llm.aquery(prompt) for prompt in prompts]\n",
    "    responses = await asyncio.gather(*tasks)\n",
    "    bhc_paragraphs = [\n",
    "        Paragraph(\n",
    "            heading=(\n",
    "                response.content\n",
    "                if response.content != \"None\" and response.content in bhc_para\n",
    "                else \"\"\n",
    "            ),\n",
    "            text=bhc_para,\n",
    "        )\n",
    "        for bhc_para, response in zip(bhc.split(\"\\n\\n\"), responses)\n",
    "    ]\n",
    "\n",
    "    num_empty_headings = sum(1 for para in bhc_paragraphs if para.heading != \"\")\n",
    "    if num_empty_headings < len(bhc_paragraphs) / 2:\n",
    "        return []\n",
    "\n",
    "    return bhc_paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "missing_bhc, missing_paragraphs = 0, 0\n",
    "idx = 0\n",
    "for hadm_id, group_df in tqdm(df.groupby(\"HADM_ID\")):\n",
    "    physician_notes = [\n",
    "        Note(\n",
    "            text=series[\"TEXT\"],\n",
    "            datetime=series[\"CHARTTIME\"],\n",
    "            category=series[\"CATEGORY\"],\n",
    "            description=series[\"DESCRIPTION\"],\n",
    "        )\n",
    "        for _, series in group_df[\n",
    "            group_df[\"CATEGORY\"] == \"Discharge summary\"\n",
    "        ].iterrows()\n",
    "    ]\n",
    "\n",
    "    discharge_summary_row = group_df[group_df[\"CATEGORY\"] == \"Discharge summary\"].iloc[\n",
    "        0\n",
    "    ]\n",
    "    bhc = extract_bhc(discharge_summary_row[\"TEXT\"])\n",
    "    bhc_paragraphs = []  # asyncio.run(extract_bhc_paragraphs(bhc))\n",
    "\n",
    "    if not bhc:\n",
    "        missing_bhc += 1\n",
    "        continue\n",
    "    if len(bhc_paragraphs) <= 1:\n",
    "        missing_paragraphs += 1\n",
    "        continue\n",
    "\n",
    "    discharge_summary = DischargeSummary(\n",
    "        text=discharge_summary_row[\"TEXT\"],\n",
    "        datetime=discharge_summary_row[\"CHARTTIME\"],\n",
    "        category=discharge_summary_row[\"CATEGORY\"],\n",
    "        description=discharge_summary_row[\"DESCRIPTION\"],\n",
    "        bhc=bhc,\n",
    "        bhc_paragraphs=bhc_paragraphs,\n",
    "    )\n",
    "\n",
    "    record = Record(\n",
    "        physician_notes=sorted(physician_notes),\n",
    "        discharge_summary=discharge_summary,\n",
    "        hadm_id=hadm_id,\n",
    "        subject_id=group_df[\"SUBJECT_ID\"].iloc[0],\n",
    "    )\n",
    "    dataset.append(record)\n",
    "len(dataset), missing_bhc, missing_paragraphs, len(df.groupby(\"HADM_ID\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = train_test_split(\n",
    "    dataset, test_size=0.2, random_state=RANDOM_SEED\n",
    ")\n",
    "len(train_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(TRAIN_SAVE_PATH, \"wb\") as out_file:\n",
    "    pickle.dump([record.dict() for record in train_dataset], out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(TEST_SAVE_PATH, \"wb\") as out_file:\n",
    "    pickle.dump([record.dict() for record in test_dataset], out_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
