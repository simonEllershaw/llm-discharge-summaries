{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "\n",
    "import torch\n",
    "from dotenv import load_dotenv\n",
    "from matplotlib import pyplot as plt\n",
    "from pydantic import BaseModel\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from discharge_summaries.openai_llm.chat_models import AzureOpenAIChatModel\n",
    "from discharge_summaries.openai_llm.message import Message, Role\n",
    "from discharge_summaries.openai_llm.token_count import (\n",
    "    num_tokens_from_messages_azure_engine,\n",
    ")\n",
    "from discharge_summaries.schemas.mimic import Note, Record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path.cwd().parent / \"data\"\n",
    "OUTPUT_DIR = Path.cwd() / \"output\"\n",
    "\n",
    "\n",
    "TRAINING_DATASET_PATH = DATA_DIR / \"train_all_ds.pkl\"\n",
    "RANDOM_SEED = 23\n",
    "AZURE_ENGINE = \"gpt-4-32k\"\n",
    "AZURE_API_VERSION = \"2023-07-01-preview\"\n",
    "# AZURE_ENGINE = \"gpt-35-turbo\"\n",
    "# AZURE_API_VERSION = \"2023-07-01-preview\"\n",
    "\n",
    "GUIDELINES_JSON_SCHEMA_PATH = (\n",
    "    Path.cwd().parent / \"guidelines\" / \"eDischarge-Summary-v2.1-1st-Feb-21_schema.json\"\n",
    ")\n",
    "LLAMA_2_CONTEXT_LENGTH = 4096\n",
    "MAX_PROMPT_LENGTH = LLAMA_2_CONTEXT_LENGTH - 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(TRAINING_DATASET_PATH, \"rb\") as in_file:\n",
    "    dataset = [Record(**record) for record in pickle.load(in_file)]\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextChunk(BaseModel):\n",
    "    text: str\n",
    "    timestamp: str\n",
    "    token_length: int\n",
    "\n",
    "\n",
    "def text_to_chunks(note: Note, max_chunk_length: int = 128) -> List[TextChunk]:\n",
    "    chunks = []\n",
    "    for section in note.text.split(\"\\n\\n\"):\n",
    "        prefix = f\"Physician Note Extract\\nTimestamp: {note.datetime}\"\n",
    "        prefix_length = num_tokens_from_messages_azure_engine(\n",
    "            [Message(content=prefix, role=Role.USER)], AZURE_ENGINE, AZURE_API_VERSION\n",
    "        )\n",
    "\n",
    "        chunk_text = prefix\n",
    "        chunk_length = prefix_length\n",
    "\n",
    "        for sentence in re.split(\"\\n(?=[^ a-z])|(?<=[?|!|.])\\\\s\", section):\n",
    "            new_chunk_text = f\"{chunk_text}\\n{sentence}\"\n",
    "            new_chunk_length = num_tokens_from_messages_azure_engine(\n",
    "                [Message(content=new_chunk_text, role=Role.USER)],\n",
    "                AZURE_ENGINE,\n",
    "                AZURE_API_VERSION,\n",
    "            )\n",
    "            if new_chunk_length > max_chunk_length:\n",
    "                chunks.append(\n",
    "                    TextChunk(\n",
    "                        text=chunk_text,\n",
    "                        timestamp=note.datetime,\n",
    "                        token_length=chunk_length,\n",
    "                    )\n",
    "                )\n",
    "                chunk_text = prefix\n",
    "                chunk_length = prefix_length\n",
    "            else:\n",
    "                chunk_text = new_chunk_text\n",
    "                chunk_length = new_chunk_length\n",
    "\n",
    "        if chunk_text != prefix:\n",
    "            chunks.append(\n",
    "                TextChunk(\n",
    "                    text=chunk_text, timestamp=note.datetime, token_length=chunk_length\n",
    "                )\n",
    "            )\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_note_chunks = [\n",
    "    [chunk for note in sample.physician_notes for chunk in text_to_chunks(note)]\n",
    "    for sample in tqdm(dataset[:1000])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_note_chunks[0][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(\n",
    "    [chunk.token_length for note_chunks in dataset_note_chunks for chunk in note_chunks]\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guidelines_json_schema_json = json.loads(GUIDELINES_JSON_SCHEMA_PATH.read_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "element_schemas_json = [\n",
    "    {\n",
    "        \"type\": \"object\",\n",
    "        \"description\": f\"{section_title}. {section_dict['description']}\",\n",
    "        \"properties\": {element_title: element_dict},\n",
    "    }\n",
    "    for section_title, section_dict in guidelines_json_schema_json[\"properties\"].items()\n",
    "    for element_title, element_dict in section_dict[\"properties\"].items()\n",
    "]\n",
    "len(element_schemas_json), element_schemas_json[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def element_json_schema_to_str(element_json_schema: Dict) -> str:\n",
    "    section_str = element_json_schema[\"description\"]\n",
    "    assert len(element_json_schema[\"properties\"].items()) == 1\n",
    "\n",
    "    element_title, element_dict = next(iter(element_json_schema[\"properties\"].items()))\n",
    "\n",
    "    if element_dict[\"type\"] == \"array\":\n",
    "        element_str = \"\\n\".join(\n",
    "            [\n",
    "                f\"{array_element_title}. {array_element_dict['description']}\"\n",
    "                for array_element_title, array_element_dict in element_dict[\n",
    "                    \"items\"\n",
    "                ].items()\n",
    "            ]\n",
    "        )\n",
    "    else:\n",
    "        element_str = f\"{element_title}. {element_dict['description']}.\"\n",
    "\n",
    "    return f\"{section_str}\\n{element_str}\"\n",
    "\n",
    "\n",
    "guideline_element_strs = [\n",
    "    element_json_schema_to_str(element_json_schema)\n",
    "    for element_json_schema in element_schemas_json\n",
    "]\n",
    "\n",
    "print(guideline_element_strs[0])\n",
    "print(guideline_element_strs[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = SentenceTransformer(\"msmarco-distilbert-base-tas-b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guideline_element_embeddings = embedder.encode(\n",
    "    guideline_element_strs, convert_to_tensor=True, show_progress_bar=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 20\n",
    "dataset_similarity_scores_cumsum = torch.ones(\n",
    "    (len(dataset_note_chunks), len(element_schemas_json), n)\n",
    ")\n",
    "\n",
    "softmax = torch.nn.Softmax(dim=1)\n",
    "for idx, note_chunks in tqdm(enumerate(dataset_note_chunks)):\n",
    "    note_chunk_embeddings = embedder.encode(\n",
    "        [chunk.text for chunk in note_chunks], convert_to_tensor=True\n",
    "    )\n",
    "    similarity_scores = util.dot_score(\n",
    "        guideline_element_embeddings, note_chunk_embeddings\n",
    "    ).cpu()\n",
    "    similarity_scores_normalized, _ = softmax(similarity_scores).sort(\n",
    "        dim=1, descending=True\n",
    "    )\n",
    "    similarity_scores_cumsum = similarity_scores_normalized.cumsum(dim=1)\n",
    "    copy_n = min(similarity_scores_cumsum.shape[-1], n)\n",
    "    dataset_similarity_scores_cumsum[idx, :, :copy_n] = similarity_scores_cumsum[\n",
    "        :, :copy_n\n",
    "    ]\n",
    "dataset_similarity_scores_cumsum.shape\n",
    "# for chunk in note_chunks:\n",
    "#     chunk.guideline_element_embeddings = guideline_element_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_similarity_scores_cumsum_merged = dataset_similarity_scores_cumsum.reshape(\n",
    "    -1, n\n",
    ")\n",
    "dataset_similarity_scores_mean = dataset_similarity_scores_cumsum_merged.mean(dim=0)\n",
    "dataset_similarity_scores_std = dataset_similarity_scores_cumsum_merged.std(dim=0)\n",
    "\n",
    "dataset_similarity_scores_mean.shape, dataset_similarity_scores_std.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(dataset_similarity_scores_cumsum_merged.mean(dim=0))\n",
    "plt.fill_between(\n",
    "    range(len(dataset_similarity_scores_std)),\n",
    "    dataset_similarity_scores_mean + dataset_similarity_scores_std,\n",
    "    dataset_similarity_scores_mean - dataset_similarity_scores_std,\n",
    "    color=\"gray\",\n",
    "    alpha=0.5,\n",
    "    label=\"Confidence Bounds\",\n",
    ")\n",
    "plt.xlabel(\"n\")\n",
    "plt.ylabel(\"Mean Cumulative Similarity Score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = AzureOpenAIChatModel(\n",
    "    api_base=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_KEY\"),\n",
    "    api_version=AZURE_API_VERSION,\n",
    "    engine=AZURE_ENGINE,\n",
    "    temperature=0,\n",
    "    timeout=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_MESSAGE = Message(\n",
    "    role=Role.SYSTEM,\n",
    "    content=\"\"\"You are a consultant doctor tasked with writing a patients discharge summary.\n",
    "Only the information in the physician notes provided by the user can be used for this task.\n",
    "Each physician note has a title of the format Physician Note [number]: [timestamp].\n",
    "\n",
    "The discharge summary must be written in accordance with the following json schema.\n",
    "guidelines_json_schema_str\n",
    "If the information is not present to fill in a field, answer it with an empty string.\n",
    "\"\"\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
