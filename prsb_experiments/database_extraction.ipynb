{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Union\n",
    "\n",
    "import jsonref\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "from dotenv import load_dotenv\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "from discharge_summaries.openai_llm.chat_models import AzureOpenAIChatModel\n",
    "from discharge_summaries.openai_llm.message import Message, Role\n",
    "from discharge_summaries.schemas.prsb_guidelines import DischargeSummary\n",
    "from discharge_summaries.structured_data_extractors.mimic import (\n",
    "    MIMICStructuredDataExtractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIMIC_III_DIR = (\n",
    "    Path.cwd().parent / \"data\" / \"physionet.org\" / \"files\" / \"mimiciii\" / \"1.4\"\n",
    ")\n",
    "AZURE_ENGINE = \"gpt-3-turbo-16k\"\n",
    "AZURE_API_VERSION = \"2023-07-01-preview\"\n",
    "TOKENIZER = tiktoken.get_encoding(\"cl100k_base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physician_notes_df = pd.read_csv(MIMIC_III_DIR / \"physician_notes.csv\")\n",
    "discharge_summary_df = pd.read_csv(MIMIC_III_DIR / \"discharge_summaries.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_data_extractor = MIMICStructuredDataExtractor(MIMIC_III_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hadm_ids = discharge_summary_df[\"HADM_ID\"].unique()\n",
    "random.Random(23).shuffle(hadm_ids)\n",
    "hadm_id = hadm_ids[0]\n",
    "hadm_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physician_notes_hadm_id_df = physician_notes_df[\n",
    "    physician_notes_df[\"HADM_ID\"] == hadm_ids[0]\n",
    "]\n",
    "len(physician_notes_hadm_id_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_data_summary = structured_data_extractor.complete_prsb_discharge_summary(\n",
    "    hadm_id\n",
    ")\n",
    "structured_data_summary_dict = structured_data_summary.dict()\n",
    "medications_structured_data = structured_data_summary_dict.pop(\n",
    "    \"medications_and_medical_devices\"\n",
    ")\n",
    "procedures_structured_data = structured_data_summary_dict.pop(\"procedures\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_key_from_schema(schema: Union[Dict, List, str], remove_key: str):\n",
    "    if isinstance(schema, dict):\n",
    "        if remove_key in schema.keys():\n",
    "            del schema[remove_key]\n",
    "        for key in schema.keys():\n",
    "            remove_key_from_schema(schema[key], remove_key)\n",
    "    elif isinstance(schema, list):\n",
    "        for item in schema:\n",
    "            remove_key_from_schema(item, remove_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_ref_dict_to_dict(json_ref_dict: Dict) -> Dict:\n",
    "    for k, v in json_ref_dict.items():\n",
    "        if type(v) == jsonref.JsonRef:\n",
    "            json_ref_dict[k] = json_ref_dict_to_dict(dict(v))\n",
    "        elif type(v) == dict:\n",
    "            json_ref_dict[k] = json_ref_dict_to_dict(v)\n",
    "    return json_ref_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_schema = jsonref.loads(DischargeSummary.schema_json(), jsonschema=True)\n",
    "json_schema = json_ref_dict_to_dict(json_schema)\n",
    "json_schema.pop(\"definitions\")\n",
    "remove_key_from_schema(json_schema, \"required\")\n",
    "# Keep top level title\n",
    "remove_key_from_schema(json_schema[\"properties\"], \"title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_previously_filled_schema_fields(schema: Dict, filled_schema: Dict) -> Dict:\n",
    "    for key, value in filled_schema.items():\n",
    "        if isinstance(value, dict):\n",
    "            schema[key][\"properties\"] = {\n",
    "                property_key: property_value\n",
    "                for property_key, property_value in remove_previously_filled_schema_fields(\n",
    "                    schema[key][\"properties\"], value\n",
    "                ).items()\n",
    "                if property_value\n",
    "            }\n",
    "        else:\n",
    "            if value:\n",
    "                del schema[key]\n",
    "    return schema\n",
    "\n",
    "\n",
    "json_schema[\"properties\"] = remove_previously_filled_schema_fields(\n",
    "    json_schema[\"properties\"], structured_data_summary_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unfillable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unfillable_sections = {\n",
    "    \"discharge_details\",\n",
    "    \"distribution_list\",\n",
    "    \"person_completing_record\",\n",
    "    \"allergies_and_adverse_reactions\",\n",
    "    \"diagnoses\",\n",
    "}\n",
    "\n",
    "for section in unfillable_sections:\n",
    "    json_schema[\"properties\"].pop(section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snomed_search_sections = {\n",
    "    \"procedures\",\n",
    "    \"medications_and_medical_devices\",\n",
    "    \"investigation_results\",\n",
    "    \"assessment_scale\",\n",
    "}  # handle allergies separately\n",
    "for section in snomed_search_sections:\n",
    "    json_schema[\"properties\"].pop(section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_schema[\"properties\"].pop(\"clinical_summary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_unique_note_sections(physician_notes_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Could be smarter here alot of text overlap\n",
    "    physician_notes_df_filtered = (\n",
    "        physician_notes_df[[\"CHARTTIME\", \"TEXT\"]]\n",
    "        .drop_duplicates()\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    prev_added_text = set()\n",
    "    note_sections = []\n",
    "    for note_idx, note in physician_notes_df_filtered.sort_values(\n",
    "        \"CHARTTIME\"\n",
    "    ).iterrows():\n",
    "        start_char_idx = 0\n",
    "        for section_text in re.split(\n",
    "            \"\\n(?=^[^\\n].*?:)\", note[\"TEXT\"], flags=re.MULTILINE\n",
    "        ):\n",
    "            end_char_idx = start_char_idx + len(section_text)\n",
    "            if section_text not in prev_added_text:\n",
    "                note_sections.append(\n",
    "                    (note_idx, start_char_idx, end_char_idx, section_text)\n",
    "                )\n",
    "                prev_added_text.add(section_text)\n",
    "            start_char_idx = end_char_idx + 1\n",
    "    return pd.DataFrame.from_records(\n",
    "        note_sections, columns=[\"note_idx\", \"start_char_idx\", \"end_char_idx\", \"text\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sections_df = extract_unique_note_sections(physician_notes_hadm_id_df)\n",
    "sections_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_lg\", disable=[\"tok2vec\", \"parser\", \"ner\"])\n",
    "except OSError:\n",
    "    spacy.cli.download(\"en_core_web_lg\")\n",
    "    nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenise_texts(\n",
    "    texts: List[str], spacy_pipeline: spacy.language.Language\n",
    ") -> List[str]:\n",
    "    return [\n",
    "        [\n",
    "            token.lemma_\n",
    "            for token in doc\n",
    "            if not token.is_punct and not token.is_stop and token.is_alpha\n",
    "        ]\n",
    "        for doc in spacy_pipeline.pipe(texts)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenised_sections = tokenise_texts(sections_df[\"text\"].tolist(), nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25 = BM25Okapi(tokenised_sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_dict = {}\n",
    "for section_name, section_values in json_schema[\"properties\"].items():\n",
    "    query_string = f'{section_name.replace(\"_\", \" \")} {section_values[\"description\"]}'\n",
    "    if section_values[\"type\"] == \"object\":\n",
    "        for property_key, property_value in section_values[\"properties\"].items():\n",
    "            query_string += (\n",
    "                f\" {property_key.replace('_', ' ')} {property_value['description']}\"\n",
    "            )\n",
    "    tokenised_query = tokenise_texts([query_string], nlp)[0]\n",
    "\n",
    "    doc_scores = bm25.get_scores(tokenised_query)\n",
    "    top_n_idxs = np.argsort(doc_scores)[-10:][::-1]\n",
    "    retrieved_sections = \"\\n\\n\".join(\n",
    "        f\"Extract {idx + 1}\\n{text}\"\n",
    "        for idx, text in enumerate(sections_df.iloc[top_n_idxs][\"text\"])\n",
    "    )\n",
    "\n",
    "    print(section_name)\n",
    "    print(retrieved_sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = (\n",
    "    \"Individual requirement that a person has. These may be a communication, cultural,\"\n",
    "    \" cognitive or mobility need.\"\n",
    ")\n",
    "tokenised_query = tokenise_texts([query], nlp)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_scores = np.array(bm25.get_scores(tokenised_query))\n",
    "top_n_idxs = np.argsort(doc_scores)[-10:][::-1]\n",
    "top_n_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, text in enumerate(sections_df.iloc[top_n_idxs][\"text\"]):\n",
    "    print(f\"Extract {idx + 1}\\n{text}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"Some text is interesting about paracetamol [[12312/12]] [Hospital 123].\"\n",
    "doc = nlp(test.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned = [\n",
    "    token.lemma_\n",
    "    for token in doc\n",
    "    if not token.is_punct and not token.is_stop and token.is_alpha\n",
    "]\n",
    "cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(TOKENIZER.encode(json.dumps(json_schema)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_notes_string(physician_notes_df: pd.DataFrame):\n",
    "    # Could be smarter here alot of text overlap\n",
    "    physician_notes_df_filtered = (\n",
    "        physician_notes_df[[\"CHARTTIME\", \"TEXT\"]]\n",
    "        .drop_duplicates()\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    added_sections = set()\n",
    "    physician_notes = []\n",
    "    for idx, note in physician_notes_df_filtered.sort_values(\"CHARTTIME\").iterrows():\n",
    "        new_sections = \"\"\n",
    "        for note_section in re.split(\n",
    "            \"\\n(?=^[^\\n].*?:)\", note[\"TEXT\"], flags=re.MULTILINE\n",
    "        ):\n",
    "            if note_section not in added_sections:\n",
    "                new_sections += \"\\n\" + note_section\n",
    "                added_sections.add(note_section)\n",
    "        physician_notes.append(\n",
    "            f\"Physician Note {idx+1}: {note['CHARTTIME']}{new_sections}\"\n",
    "        )\n",
    "    return \"\\n\\n\".join(physician_notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_system_message(json_schema: Dict) -> Message:\n",
    "    return Message(\n",
    "        role=Role.SYSTEM,\n",
    "        content=f\"\"\"You are a consultant doctor tasked with writing a patients discharge summary.\n",
    "Only the information in the physician notes provided by the user can be used for this task.\n",
    "Each physician note has a title of the format Physician Note [number]: [timestamp].\n",
    "\n",
    "The discharge summary must be written in accordance with the following json schema.\n",
    "{json.dumps(json_schema)}\n",
    "If the information is not present to fill in a field, answer it with an empty string or list.\n",
    "\"\"\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = AzureOpenAIChatModel(\n",
    "    api_base=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_KEY\"),\n",
    "    api_version=AZURE_API_VERSION,\n",
    "    engine=AZURE_ENGINE,\n",
    "    temperature=0,\n",
    "    timeout=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_discharge_summary(\n",
    "    physician_note_df: pd.DataFrame,\n",
    "    json_schema: Dict,\n",
    "    llm: AzureOpenAIChatModel,\n",
    "    max_prompt_tokens=15000,\n",
    ") -> List[Message]:\n",
    "    system_message = create_system_message(json_schema)\n",
    "\n",
    "    notes_string = generate_notes_string(physician_note_df)\n",
    "    user_message_content = (\n",
    "        \"Generate the discharge summary json given the following physician\"\n",
    "        f\" notes\\n\\n{notes_string}\"\n",
    "    )\n",
    "    prompt_messages = [\n",
    "        system_message,\n",
    "        Message(role=Role.USER, content=user_message_content),\n",
    "    ]\n",
    "\n",
    "    num_prompt_tokens = sum(\n",
    "        len(TOKENIZER.encode(message.content)) for message in prompt_messages\n",
    "    )\n",
    "    print(num_prompt_tokens)\n",
    "    if num_prompt_tokens > max_prompt_tokens:\n",
    "        raise ValueError(\n",
    "            f\"Prompt has {num_prompt_tokens} tokens, which is greater than the max of\"\n",
    "            f\" {max_prompt_tokens}.\"\n",
    "        )\n",
    "\n",
    "    return prompt_messages + [llm.query(prompt_messages)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = generate_discharge_summary(physician_notes_hadm_id_df, json_schema, llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = Path.cwd() / \"output\" / f\"mimic_hadm_id_{int(hadm_id)}\"\n",
    "if not output_path.exists():\n",
    "    output_path.mkdir(parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(output_path / \"json_schema.json\").write_text(json.dumps(json_schema, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_notes = \"\\n\\n\".join(\n",
    "    f\"Physician Note {idx+1}: {note['CHARTTIME']}\\n{note['TEXT']}\"\n",
    "    for idx, note in physician_notes_hadm_id_df.sort_values(\"CHARTTIME\").iterrows()\n",
    ")\n",
    "(output_path / \"physician_notes.txt\").write_text(combined_notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(output_path / \"discharge_summary.json\").write_text(\n",
    "    json.dumps(json.loads(messages[-1].content), indent=4)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(output_path / \"prompts.json\").write_text(\n",
    "    json.dumps([message.dict() for message in messages[:-1]], indent=4)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mimic_hadm_id_ds = discharge_summary_df[discharge_summary_df[\"HADM_ID\"] == hadm_id][\n",
    "    \"TEXT\"\n",
    "].iloc[0]\n",
    "(output_path / \"mimic_discharge_summary.txt\").write_text(mimic_hadm_id_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medications_structured_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
