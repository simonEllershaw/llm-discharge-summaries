{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Set\n",
    "\n",
    "# from langchain.chains import LLMChain\n",
    "# from langchain.chat_models import AzureChatOpenAI\n",
    "from medcat.cat import CAT\n",
    "from spacy.pipeline import Sentencizer\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# from discharge_summaries.prompts.diagnosis_summary import diagnosis_summary_prompt\n",
    "from discharge_summaries.schemas.medcat import MedCATSpan\n",
    "from discharge_summaries.schemas.mimic import Note, Record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path.cwd().parent / \"data\"\n",
    "DATA_PATH = DATA_DIR / \"train.pkl\"\n",
    "\n",
    "MODEL_PATH = Path.cwd().parent / \"models\" / \"medcat_model_pack_328be3555e2c7a37\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATA_PATH, \"rb\") as in_file:\n",
    "    dataset = [Record(**record) for record in pickle.load(in_file)]\n",
    "dataset = dataset[:5]\n",
    "len(dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "https://lhncbc.nlm.nih.gov/ii/tools/MetaMap/Docs/SemanticTypes_2018AB.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = CAT.load_model_pack(MODEL_PATH)\n",
    "# type_ids_filter = [\n",
    "#     \"T020\",\n",
    "#     \"T190\",\n",
    "#     \"T049\",\n",
    "#     \"T019\",\n",
    "#     \"T047\",\n",
    "#     \"T050\",\n",
    "#     \"T033\",\n",
    "#     \"T037\",\n",
    "#     \"T048\",\n",
    "#     \"T191\",\n",
    "#     \"T046\",\n",
    "#     \"T184\",\n",
    "# ] + [\"T005\", \"T007\"]\n",
    "# cui_filters = {\n",
    "#     cui\n",
    "#     for type_ids in type_ids_filter\n",
    "#     for cui in cat.cdb.addl_info[\"type_id2cuis\"][type_ids]\n",
    "# }\n",
    "# cat.cdb.config.linking[\"filters\"][\"cuis\"] = cui_filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat.pipe.spacy_nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat(dataset[0].discharge_summary.bhc_paragraphs[0].text).ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat.pipe._nlp.add_pipe(\n",
    "    \"sentencizer\", config={\"punct_chars\": Sentencizer.default_punct_chars.append(\"\\n\")}\n",
    ")\n",
    "cat.pipe._nlp.enable_pipe(\"sentencizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat.pipe.spacy_nlp.pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_medcat_entities(text: str, cat: CAT, datetime: str) -> List[MedCATSpan]:\n",
    "    doc = cat(text)\n",
    "    sentences = list(doc.sents)\n",
    "    num_context_sentences = 1\n",
    "    total_context_sentences = 2 * num_context_sentences + 1\n",
    "    ents = []\n",
    "    for sent_idx, sent in enumerate(doc.sents):\n",
    "        if total_context_sentences > len(sentences):\n",
    "            context = sentences\n",
    "        if sent_idx - num_context_sentences < 0:\n",
    "            context = sentences[:total_context_sentences]\n",
    "        elif sent_idx + num_context_sentences + 1 > len(sentences):\n",
    "            context = sentences[-total_context_sentences:]\n",
    "        else:\n",
    "            context = sentences[\n",
    "                sent_idx - num_context_sentences : sent_idx + num_context_sentences + 1\n",
    "            ]\n",
    "        context_str = \" \".join([str(sent) for sent in context])\n",
    "        for ent in sent.ents:\n",
    "            ents.append(\n",
    "                MedCATSpan.from_spacy_span(\n",
    "                    ent, cat, context=context_str, datetime=datetime\n",
    "                )\n",
    "            )\n",
    "    return ents\n",
    "\n",
    "\n",
    "def group_entities_by_cui(entities: List[MedCATSpan]) -> Dict[str, List[MedCATSpan]]:\n",
    "    cui_to_entities: Dict[str, List[MedCATSpan]] = defaultdict(list)\n",
    "    for entity in entities:\n",
    "        cui_to_entities[entity.cui].append(entity)\n",
    "    return cui_to_entities\n",
    "\n",
    "\n",
    "def string_to_word_set(string: str) -> Set[str]:\n",
    "    return set(re.findall(r\"\\w+\", string.lower()))\n",
    "\n",
    "\n",
    "def de_duplicate_entities_based_on_content_overlap(\n",
    "    entities: List[MedCATSpan], threshold=0.75\n",
    ") -> List[MedCATSpan]:\n",
    "    unique_entities_and_set: List[tuple[MedCATSpan, Set[str]]] = []\n",
    "\n",
    "    for entity in entities:\n",
    "        is_duplicate = False\n",
    "        entity_set = string_to_word_set(entity.context)\n",
    "        for _, unique_entity_set in unique_entities_and_set:\n",
    "            overlap = len(entity_set.intersection(unique_entity_set)) / len(\n",
    "                unique_entity_set\n",
    "            )\n",
    "            if overlap > threshold:\n",
    "                is_duplicate = True\n",
    "                break\n",
    "\n",
    "        if not is_duplicate:\n",
    "            unique_entities_and_set.append((entity, entity_set))\n",
    "\n",
    "    return [entity for entity, _ in unique_entities_and_set]\n",
    "\n",
    "\n",
    "def extract_cui_to_entities(\n",
    "    physician_notes: List[Note], cat: CAT\n",
    ") -> Dict[str, List[MedCATSpan]]:\n",
    "    entities = [\n",
    "        ent\n",
    "        for note in physician_notes\n",
    "        if note.category == \"Physician \"\n",
    "        for ent in annotate_medcat_entities(note.text, cat, datetime=note.datetime)\n",
    "    ]\n",
    "\n",
    "    cui_to_entities = group_entities_by_cui(entities)\n",
    "\n",
    "    cui_to_entities = {\n",
    "        cui: de_duplicate_entities_based_on_content_overlap(entities)\n",
    "        for cui, entities in cui_to_entities.items()\n",
    "    }\n",
    "\n",
    "    return cui_to_entities\n",
    "\n",
    "\n",
    "def cui_to_name(cui: str, cat: CAT) -> str:\n",
    "    return cat.cdb.get_name(cui)\n",
    "\n",
    "\n",
    "def cuis_to_names(cuis: List[str], cat: CAT) -> List[str]:\n",
    "    return sorted(cat.cdb.get_name(cui) for cui in cuis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracts = [\n",
    "    extract_cui_to_entities(sample.physician_notes, cat) for sample in tqdm(dataset)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heading_cuis = [ for ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# timestamp_str = datetime.now().strftime(\"%Y_%m_%d_%H_%M\")\n",
    "# output_path = OUTPUT_DIR / (timestamp_str + \".json\")\n",
    "\n",
    "# with output_path.open(\"w\") as fout:\n",
    "#     json.dump([[para.Dict() for para in bhc] for bhc in generations], fout, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UMLS Matching (legacy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "formatted_summaries = []\n",
    "for summary in output.split(\"\\n\\n\"):\n",
    "    words = summary.split(\" \")\n",
    "    lines = [\" \".join(words[i : i + 10]) for i in range(0, len(words), 10)]\n",
    "    formatted_summaries.append(\"\\n\".join(lines))\n",
    "\n",
    "(DATA_DIR / \"test_output.txt\").write_text(\"\\n\\n\".join(formatted_summaries))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def calc_matches_including_parents(cuis: set[str], cuis_compare: set[str], cat: CAT)->List[str]:\n",
    "    cuis_compare_parents = {parent_cui for cui in cuis_compare for parent_cui in get_related_cuis(cui, cat, relation_labels=['CHD','RN'])}\n",
    "    matched = []\n",
    "    for cui in cuis:\n",
    "        if cui in cuis_compare:\n",
    "            matched.append(cui)\n",
    "        elif cui in cuis_compare_parents:\n",
    "            matched.append(cui)\n",
    "        elif len(get_related_cuis(cui, cat, relation_labels=['CHD','RN']).intersection(cuis_compare)) > 0:\n",
    "            matched.append(cui)\n",
    "        else:\n",
    "            print(cui_to_name(cui, cat))\n",
    "            print(cuis_to_names(cuis_compare_parents, cat))\n",
    "            print(cuis_to_names(get_related_cuis(cui, cat, relation_labels=['CHD','RN']), cat))\n",
    "\n",
    "    return matched"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "matched = []\n",
    "cuis_true_parents = {parent_cui for cui in cui_true for parent_cui in get_related_cuis(cui, cat, relation_labels=['CHD','RN'])}\n",
    "for cui_pred in cuis_pred:\n",
    "    if cui_pred in cuis_true or cui_pred in cuis_true_parents or get_related_cuis(cui_pred, cat, relation_labels=['CHD','RN']).intersection(cuis_true):\n",
    "        matched.append(cui_pred)\n",
    "\n",
    "len(matched)/len(cui_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "matches = base_matches.union(parent_true_matches).union(parent_pred_matches)\n",
    "cuis_to_names(matches, cat), cuis_to_names(cuis_true, cat), cuis_to_names(cuis_pred, cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def precision(cuis_true: set[str], cuis_pred: set[str])->float:\n",
    "    cui_true_w_parents = cuis_true.union({parent_cui for cui in cui_true for parent_cui in get_related_cuis(cui, relation_labels=['CHD','RN'])})\n",
    "    true_positive = 0\n",
    "    print(cuis_to_names(cui_true_w_parents, cat))\n",
    "    for cui_pred in cuis_pred:\n",
    "        cui_pred_w_parents = set(cui_pred).union(get_related_cuis(cui_pred, relation_labels=['CHD','RN']))\n",
    "        if cui_pred_w_parents.intersection(cui_true_w_parents):\n",
    "            print(cui_to_name(cui_pred, cat), cuis_to_names(cui_pred_w_parents.intersection(cui_true_w_parents), cat))\n",
    "            true_positive += 1\n",
    "\n",
    "    return true_positive/len(cui_pred)\n",
    "\n",
    "precision(cui_true, cui_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import requests\n",
    "\n",
    "def get_related_cuis(cui:str, cat:CAT, relation_labels:List[str]|None=None, api_key:str=UMLS_API_KEY, umls_base_url:str=UMLS_BASE_URL, page_size:int=1000)->set[str]:\n",
    "    url = f\"{umls_base_url}/content/current/CUI/{cui}/relations\"\n",
    "    relation_label_str = \",\".join(relation_labels) if relation_labels else \"\"\n",
    "    params = {\"apiKey\": api_key, \"includeRelationLabels\": relation_label_str, \"pageNumber\": 1, \"pageSize\": page_size}\n",
    "    results = []\n",
    "    completed = False\n",
    "    while not completed:\n",
    "        response = requests.get(url, params)\n",
    "        if response.status_code == 200:\n",
    "            response_json = response.json()\n",
    "            results.extend(response_json[\"result\"])\n",
    "            params[\"pageNumber\"] += 1\n",
    "            if response_json[\"pageCount\"] == 1:\n",
    "                completed = True\n",
    "        else:\n",
    "            print(f\"Failed to retrieve concept information for CUI: {cui}. Error code {response.status_code}\")\n",
    "            completed = True\n",
    "\n",
    "    related_cuis = {result[\"relatedId\"].split(\"/\")[-1] for result in results if \"relatedId\" in result}\n",
    "    return {cui for cui in related_cuis if cui in cat.cdb.cui2names}\n",
    "\n",
    "def get_child_to_parent_cui(org_cuis: List[str], cat)->Dict[str, str]:\n",
    "    child_to_parent_cui = {}\n",
    "    for cui in org_cuis:\n",
    "        parent_cuis = get_related_cuis(cui, cat, relation_labels=['CHD','RN'])\n",
    "        parent_cuis = parent_cuis.intersection(org_cuis)\n",
    "        if parent_cuis:\n",
    "            child_to_parent_cui[cui] = next(iter(parent_cuis))\n",
    "\n",
    "    child_to_extend_parent_cui = {}\n",
    "    for child_cui in child_to_parent_cui.keys():\n",
    "        parent_exists = True\n",
    "        extend_parent_cui = child_cui\n",
    "        while parent_exists:\n",
    "            if extend_parent_cui in child_to_parent_cui:\n",
    "                extend_parent_cui = child_to_parent_cui[extend_parent_cui]\n",
    "            else:\n",
    "                parent_exists = False\n",
    "        if extend_parent_cui != child_cui:\n",
    "            child_to_extend_parent_cui[child_cui] = extend_parent_cui\n",
    "    return child_to_extend_parent_cui\n",
    "\n",
    "child_to_parent_cui = get_child_to_parent_cui(cui_to_entities.keys(), cat)\n",
    "{cui_to_name(child_cui, cat): cui_to_name(parent_cui, cat) for child_cui, parent_cui in child_to_parent_cui.items()}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
